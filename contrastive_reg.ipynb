{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAdY3DB2gpVY"
      },
      "source": [
        "## Google colab setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epTU0mbHgpVa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IS_COLAB = True\n",
        "except:\n",
        "  IS_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoCoM19EgpVd",
        "outputId": "0f9dcb7a-c8c1-4395-a89b-5b9766b062c8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow_addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.20.0 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "if IS_COLAB:\n",
        "  !pip install tensorflow_addons\n",
        "  !pip install unidecode\n",
        "  !pip install transformers\n",
        "  # !pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6QAQOrigpVf",
        "outputId": "2d7c78e3-cff6-4a08-baca-33adc0147b92",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZewVEh8GgpVf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/SemiSupervised/*.csv ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELhYr27UgpVg"
      },
      "source": [
        "## Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9RHrZPXIgpVg",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-29 14:14:42.175573: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-29 14:14:42.690241: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64::/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:\n",
            "2023-04-29 14:14:42.690285: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64::/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:\n",
            "2023-04-29 14:14:42.690290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-04-29 14:14:43.327949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-29 14:14:43.328294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-29 14:14:43.332242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-29 14:14:43.332575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-29 14:14:43.332888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-29 14:14:43.333204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import spacy\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NNyA9_DfgpVi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "tf.config.set_visible_devices(gpus[1], 'GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J10vZgj5gpVi"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ATcVYFIzgpVj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"train_ner.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OY7jHZ-FgpVk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def preprocess(x):\n",
        "    s = unidecode(x)\n",
        "    s = str.lower(s)\n",
        "    s = re.sub(r\"\\[[a-z]+\\]\",\"\", s)\n",
        "    s = re.sub(r\"\\*\",\"\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z0-9]+\",\" \",s)\n",
        "    s = re.sub(r\" +\",\" \",s)\n",
        "    s = re.sub(r\"(.)\\1+\",r\"\\1\",s)\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzuBpbLWgpVl",
        "outputId": "772b4513-a970-4aea-9444-8e3b1b99410b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ABUSE': 0, 'INSULT': 1, 'OTHER': 2, 'PROFANITY': 3}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_ids = {label_name:i for i, label_name in enumerate(sorted(set(data[\"label\"])))}\n",
        "label_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "puCrlbe2gpVl",
        "outputId": "cc336f49-6bd8-4d96-8162-332ce4b61c36",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32674</td>\n",
              "      <td>da de unde stii u mai [ORG] ca banii au fost p...</td>\n",
              "      <td>ABUSE</td>\n",
              "      <td>da de unde sti u mai ca bani au fost pt si nu ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16514</td>\n",
              "      <td>m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...</td>\n",
              "      <td>INSULT</td>\n",
              "      <td>muie muie muie bai kakatule stai in banca ta d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32556</td>\n",
              "      <td>PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>pt ala care are treaba cu esti unul care nu ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23861</td>\n",
              "      <td>sunt bucuros ca [PERS] nu a mai venit la [ORG]...</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>sunt bucuros ca nu a mai venit la jucator de d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21811</td>\n",
              "      <td>[PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...</td>\n",
              "      <td>INSULT</td>\n",
              "      <td>esti pe n es cu li bi l te asemeni cu de la c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                               text   label  \\\n",
              "0  32674  da de unde stii u mai [ORG] ca banii au fost p...   ABUSE   \n",
              "1  16514  m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...  INSULT   \n",
              "2  32556  PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...   OTHER   \n",
              "3  23861  sunt bucuros ca [PERS] nu a mai venit la [ORG]...   OTHER   \n",
              "4  21811  [PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...  INSULT   \n",
              "\n",
              "                                        preprocessed  \n",
              "0  da de unde sti u mai ca bani au fost pt si nu ...  \n",
              "1  muie muie muie bai kakatule stai in banca ta d...  \n",
              "2  pt ala care are treaba cu esti unul care nu ar...  \n",
              "3  sunt bucuros ca nu a mai venit la jucator de d...  \n",
              "4   esti pe n es cu li bi l te asemeni cu de la c...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"preprocessed\"] = data[\"text\"].apply(preprocess)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "OkPqrqdAgpVm",
        "outputId": "937749db-1cf9-408c-d323-ab6a24cea79d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>preprocessed</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32674</td>\n",
              "      <td>da de unde stii u mai [ORG] ca banii au fost p...</td>\n",
              "      <td>ABUSE</td>\n",
              "      <td>da de unde sti u mai ca bani au fost pt si nu ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16514</td>\n",
              "      <td>m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...</td>\n",
              "      <td>INSULT</td>\n",
              "      <td>muie muie muie bai kakatule stai in banca ta d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32556</td>\n",
              "      <td>PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>pt ala care are treaba cu esti unul care nu ar...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23861</td>\n",
              "      <td>sunt bucuros ca [PERS] nu a mai venit la [ORG]...</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>sunt bucuros ca nu a mai venit la jucator de d...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21811</td>\n",
              "      <td>[PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...</td>\n",
              "      <td>INSULT</td>\n",
              "      <td>esti pe n es cu li bi l te asemeni cu de la c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                               text   label  \\\n",
              "0  32674  da de unde stii u mai [ORG] ca banii au fost p...   ABUSE   \n",
              "1  16514  m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...  INSULT   \n",
              "2  32556  PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...   OTHER   \n",
              "3  23861  sunt bucuros ca [PERS] nu a mai venit la [ORG]...   OTHER   \n",
              "4  21811  [PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...  INSULT   \n",
              "\n",
              "                                        preprocessed  class  \n",
              "0  da de unde sti u mai ca bani au fost pt si nu ...      0  \n",
              "1  muie muie muie bai kakatule stai in banca ta d...      1  \n",
              "2  pt ala care are treaba cu esti unul care nu ar...      2  \n",
              "3  sunt bucuros ca nu a mai venit la jucator de d...      2  \n",
              "4   esti pe n es cu li bi l te asemeni cu de la c...      1  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"class\"] = data[\"label\"].map(lambda x: label_ids[x])\n",
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vhnskykgpVm",
        "outputId": "99097769-aed0-42a9-b63d-04ff650b4dd4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OTHER        3649\n",
              "ABUSE        2768\n",
              "INSULT       2242\n",
              "PROFANITY    1294\n",
              "Name: label, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCSuJiBzgpVn"
      },
      "source": [
        "## Prepare data splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6teKcVuJgpVn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-29 14:14:48.285796: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-29 14:14:48.286374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-29 14:14:48.286709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-29 14:14:48.286992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-29 14:14:48.669999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-29 14:14:48.670326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-29 14:14:48.670599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-04-29 14:14:48.670842: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-04-29 14:14:48.670864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6646 MB memory:  -> device: 1, name: GeForce RTX 2070 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
          ]
        }
      ],
      "source": [
        "tok_robert = AutoTokenizer.from_pretrained(\"readerbench/RoBERT-base\")\n",
        "robert = TFAutoModel.from_pretrained(\"readerbench/RoBERT-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SGPTE7pOgpVo",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def split_ssl_data(ids_array,mask_array,labels,num_classes,label_percent):\n",
        "  labeled = None\n",
        "  unlabeled = None\n",
        "\n",
        "  for class_idx in range(num_classes):\n",
        "    class_ids = ids_array[labels==class_idx]\n",
        "    class_mask = mask_array[labels==class_idx]\n",
        "    sz = int(label_percent * class_ids.shape[0])\n",
        "\n",
        "    labels_reduced = labels[labels==class_idx][:sz]\n",
        "    labeled_ids, unlabeled_ids = class_ids[:sz], class_ids[sz:]\n",
        "    labeled_mask, unlabeled_mask = class_mask[:sz], class_mask[sz:]\n",
        "\n",
        "    if not labeled:\n",
        "      labeled = (labeled_ids, labeled_mask, labels_reduced)\n",
        "      unlabeled = (unlabeled_ids, unlabeled_mask)\n",
        "    else:\n",
        "      labeled = (\n",
        "          np.concatenate([labeled[0],labeled_ids]),\n",
        "          np.concatenate([labeled[1],labeled_mask]),\n",
        "          np.concatenate([labeled[2],labels_reduced])\n",
        "      )\n",
        "      unlabeled = (\n",
        "          np.concatenate([unlabeled[0],unlabeled_ids]),\n",
        "          np.concatenate([unlabeled[1],unlabeled_mask]),\n",
        "      )\n",
        "\n",
        "  return labeled, unlabeled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "13aGHiCOgpVo",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def preprocess_robert(x):\n",
        "  t = tok_robert(x,padding=\"max_length\",max_length=96,truncation=True,return_tensors='np')\n",
        "  return t[\"input_ids\"], t[\"attention_mask\"]\n",
        "\n",
        "def map_func(input_ids, masks, labels):\n",
        "  return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
        "\n",
        "def map_func2(input_ids, masks):\n",
        "  return {'input_ids': input_ids, 'attention_mask': masks}\n",
        "\n",
        "def prepare_ds(filename,batch_size=64):\n",
        "  df = pd.read_csv(filename)\n",
        "  X_id_mask = df['text'].map(preprocess).apply(preprocess_robert).apply(pd.Series)\n",
        "\n",
        "  X_id_mask.columns = [\"input_ids\",\"attention_mask\"]\n",
        "\n",
        "  ids_array = np.squeeze(np.stack(X_id_mask.input_ids.values), axis=1)\n",
        "  mask_array = np.squeeze(np.stack(X_id_mask.attention_mask.values), axis=1)\n",
        "  labels = df[\"label\"].map(lambda x: label_ids[x]).values\n",
        "\n",
        "  res_ds = tf.data.Dataset.from_tensor_slices((ids_array, mask_array, labels)).map(map_func).shuffle(len(df)).batch(batch_size)\n",
        "  return res_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jMuXkEkDgpVp",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def prepare_train_ds(filename,batch_size=16,label_percent=0.2):\n",
        "  df = pd.read_csv(filename)\n",
        "  df = df.sample(frac=1)\n",
        "  X_id_mask = df['text'].map(preprocess).apply(preprocess_robert).apply(pd.Series)\n",
        "\n",
        "  X_id_mask.columns = [\"input_ids\",\"attention_mask\"]\n",
        "\n",
        "  ids_array = np.squeeze(np.stack(X_id_mask.input_ids.values), axis=1)\n",
        "  mask_array = np.squeeze(np.stack(X_id_mask.attention_mask.values), axis=1)\n",
        "  labels = df[\"label\"].map(lambda x: label_ids[x]).values\n",
        "\n",
        "  labeled, unlabeled = split_ssl_data(ids_array,mask_array,labels,len(label_ids),label_percent)\n",
        "  labeled_ds = tf.data.Dataset.from_tensor_slices(labeled)\n",
        "  labeled_ds = labeled_ds.map(map_func).shuffle(len(labeled_ds)).batch(batch_size).repeat()\n",
        "  unlabeled_ds = tf.data.Dataset.from_tensor_slices(unlabeled)\n",
        "  unlabeled_ds = unlabeled_ds.map(map_func2).shuffle(len(unlabeled_ds)).batch(batch_size).repeat()\n",
        "  \n",
        "  return labeled_ds, unlabeled_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dTFlVZhygpVp",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "labeled_ds, unlabeled_ds = prepare_train_ds(\"train_ner.csv\")\n",
        "test_ds = prepare_ds(\"test_ner.csv\")\n",
        "val_ds = prepare_ds(\"validation_internal_ner.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wWCe2qYgpVq"
      },
      "source": [
        "## Model definition and declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KPWTrY-fgpVq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class FixMatchCR(tf.keras.Model):\n",
        "  def __init__(self,bert_model,num_classes=4,**kwargs):\n",
        "    super(FixMatchCR,self).__init__(**kwargs)\n",
        "    self.bert = bert_model\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "    self.weak_augment = tf.keras.layers.GaussianNoise(stddev=0.5)\n",
        "    self.strong_augment = tf.keras.layers.GaussianNoise(stddev=5)\n",
        "\n",
        "    self.cls_head = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(256,activation=\"relu\"),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(64,activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(self.num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    ids, mask = inputs\n",
        "    \n",
        "    embeds = self.bert(input_ids=ids, attention_mask=mask,training=training).pooler_output\n",
        "\n",
        "    strongs = self.strong_augment(embeds,training=training)\n",
        "    weaks = self.weak_augment(embeds,training=training)\n",
        "\n",
        "    strong_preds = self.cls_head(strongs,training=training)\n",
        "    weak_preds = self.cls_head(weaks,training=training)\n",
        "\n",
        "    return weak_preds, strong_preds, strongs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hYtbpbsvkgl3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "proj_head = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(256,activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(256)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pfaURTnkgpVq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = FixMatchCR(bert_model=robert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKa3fRppgpVr"
      },
      "source": [
        "## Train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bcxYDlq7gpVs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "optim = tfa.optimizers.AdamW(weight_decay=0.001,learning_rate=0.005)\n",
        "optim_proj = tfa.optimizers.AdamW(weight_decay=0.000,learning_rate=0.001)\n",
        "optim2 = tfa.optimizers.AdamW(weight_decay=0.0,learning_rate=0.00001)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "val_prec_metric = tf.keras.metrics.Precision(name=\"precision\")\n",
        "val_recall_metric = tf.keras.metrics.Recall(name=\"recall\")\n",
        "f1_metric_micro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='micro', name='f1_micro')\n",
        "f1_metric_macro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='macro', name='f1_macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "J2BnmBkggpVs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "thresh = 0.7\n",
        "thresh2 = 0.7\n",
        "tau = 1.0\n",
        "contrastive_weight = 1.0\n",
        "unsup_weight = 1.0\n",
        "num_classes=4\n",
        "steps_per_epoch = 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yk65ez11gpVs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# @tf.function\n",
        "# def train_step(x,y,xu):\n",
        "\n",
        "#   with tf.GradientTape() as tape:\n",
        "#       wl,_,_ = model([x[\"input_ids\"],x[\"attention_mask\"]],training=True)\n",
        "#       wu,su,sembeds = model([xu[\"input_ids\"],xu[\"attention_mask\"]],training=True)\n",
        "      \n",
        "#       zu = proj_head(sembeds, training=True)\n",
        "\n",
        "#       tu = tf.math.l2_normalize(zu,axis=1)\n",
        "\n",
        "#       pseudo_labels = tf.reduce_max(wu,axis=1)\n",
        "#       pseudo_idxs = tf.cast(tf.argmax(wu, axis=1),tf.int32)\n",
        "\n",
        "#       sims = tf.exp(tu @ tf.transpose(tu) / tau)\n",
        "#       cnt = sims.shape[0]\n",
        "#       sims = tf.linalg.set_diag(sims, tf.zeros(cnt))\n",
        "#       sums = tf.reshape(tf.reduce_sum(sims,axis=1),(-1,1))\n",
        "#       ss = sims/sums\n",
        "#       norm_sims = tf.where(tf.less(ss,1e-7), 1e-7,tf.math.log(ss))\n",
        "#       norm_sims = tf.where(tf.math.is_nan(norm_sims),1e-7,norm_sims)\n",
        "  \n",
        "#       prods = tf.linalg.set_diag(norm_sims, tf.zeros(cnt))\n",
        "\n",
        "#       a = tf.map_fn(lambda idx: tf.equal(pseudo_idxs,idx),tf.range(num_classes),fn_output_signature=bool)\n",
        "#       mask1 = tf.linalg.set_diag(tf.gather(a,pseudo_idxs),tf.zeros(cnt,dtype=bool))\n",
        "\n",
        "#       b = prods * tf.cast(mask1, tf.float32)\n",
        "\n",
        "#       p_cardinal = tf.cast(tf.math.count_nonzero(mask1, axis=1),tf.float32)\n",
        "      \n",
        "#       r = -tf.reduce_sum(b,axis=1)/p_cardinal\n",
        "#       r = tf.where(tf.math.is_nan(r),0.0,r)\n",
        "\n",
        "#       mask2 = pseudo_labels>thresh2\n",
        "\n",
        "#       lcr = tf.reduce_mean(r * tf.cast(mask2,float))\n",
        "      \n",
        "#       ls = loss_fn(y, wl)\n",
        "#       mask = pseudo_labels>thresh\n",
        "#       wu = wu[mask]\n",
        "#       su = su[mask]\n",
        "#       lu = loss_fn(tf.argmax(wu,axis=1),su)\n",
        "\n",
        "#       tf.print(ls, lu, lcr)\n",
        "#       tf.print(wl)\n",
        "\n",
        "#       loss = ls + unsup_weight * lu + contrastive_weight * lcr\n",
        "\n",
        "#   grads = tape.gradient(loss, [model.cls_head.trainable_weights, model.bert.trainable_weights, proj_head.trainable_weights])\n",
        "#   tf.print(grads)\n",
        "#   optim.apply_gradients(zip(grads[0], model.cls_head.trainable_weights))\n",
        "#   optim2.apply_gradients(zip(grads[1], model.bert.trainable_weights))\n",
        "#   optim_proj.apply_gradients(zip(grads[2], proj_head.trainable_weights))\n",
        "\n",
        "#   return loss\n",
        "\n",
        "@tf.function\n",
        "def train_step(x,y,xu):\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      wl,_,_ = model([x[\"input_ids\"],x[\"attention_mask\"]],training=True)\n",
        "      wu,su,sembeds = model([xu[\"input_ids\"],xu[\"attention_mask\"]],training=True)\n",
        "      \n",
        "      zu = proj_head(sembeds, training=True)\n",
        "\n",
        "      tu = tf.math.l2_normalize(zu,axis=1)\n",
        "\n",
        "      cnt = tf.shape(wu)[0]\n",
        "      pseudo_labels = tf.reduce_max(wu,axis=1)\n",
        "      pseudo_idxs = tf.argmax(wu, axis=1)\n",
        "      mask_labels = tf.cast(tf.equal(pseudo_idxs[:,tf.newaxis], pseudo_idxs[tf.newaxis,:]),tf.float32)\n",
        "\n",
        "      sims = tf.exp(tu @ tf.transpose(tu) / tau)\n",
        "      mask_self = tf.linalg.set_diag(tf.ones((cnt,cnt)),tf.zeros(cnt))\n",
        "      sims = sims * mask_self\n",
        "      sums = tf.reduce_sum(sims,axis=1,keepdims=True)\n",
        "      sims = sims/sums\n",
        "      sims = sims * mask_labels\n",
        "      counts = tf.math.count_nonzero(sims, axis=1)\n",
        "      mask_not_all_zero = counts > 0\n",
        "      sims = sims[mask_not_all_zero]\n",
        "      counts = tf.cast(counts[mask_not_all_zero],tf.float32)\n",
        "      sims = tf.where(tf.equal(sims,0.0),1.0,sims)\n",
        "      ru = -tf.reduce_sum(tf.math.log(sims),axis=1)/counts\n",
        "\n",
        "      mask2 = pseudo_labels[mask_not_all_zero] > thresh2\n",
        "      lcr = 0.0\n",
        "      ru = ru[mask2]\n",
        "      if tf.shape(ru)[0] != 0:\n",
        "        lcr = tf.reduce_mean(ru[mask2])\n",
        "\n",
        "      \n",
        "      ls = loss_fn(y, wl)\n",
        "      mask = pseudo_labels>thresh\n",
        "      wu = wu[mask]\n",
        "      su = su[mask]\n",
        "      lu = loss_fn(tf.argmax(wu,axis=1),su)\n",
        "\n",
        "      loss = ls + unsup_weight * lu + contrastive_weight * lcr\n",
        "\n",
        "  grads = tape.gradient(loss, [model.cls_head.trainable_weights, model.bert.trainable_weights, proj_head.trainable_weights])\n",
        "  optim.apply_gradients(zip(grads[0], model.cls_head.trainable_weights))\n",
        "  optim2.apply_gradients(zip(grads[1], model.bert.trainable_weights))\n",
        "  optim_proj.apply_gradients(zip(grads[2], proj_head.trainable_weights))\n",
        "\n",
        "  return loss\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    wpred,_,_ = model([x[\"input_ids\"],x[\"attention_mask\"]], training=False)\n",
        "    val_acc_metric.update_state(y, wpred)\n",
        "    true_hot = tf.one_hot(y, 4)\n",
        "    val_prec_metric.update_state(true_hot, wpred)\n",
        "    val_recall_metric.update_state(true_hot, wpred)\n",
        "    f1_metric_micro.update_state(true_hot, wpred)\n",
        "    f1_metric_macro.update_state(true_hot, wpred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnI1Wj_RgpVs",
        "outputId": "4075b949-8632-4a39-d4e5-1c2d0a9f92d8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Training loss (for one batch) at step 0: 3.6965\n",
            "Training loss (for one batch) at step 100: 1.1331\n",
            "Training loss (for one batch) at step 200: 1.2376\n",
            "Training loss (for one batch) at step 300: 2.4034\n",
            "Validation acc: 0.562 precision: 0.598 recall: 0.531 f1_micro: 0.563 f1_macro: 0.340\n",
            "Time taken: 165.25s\n",
            "Epoch 1\n",
            "Training loss (for one batch) at step 0: 4.5221\n",
            "Training loss (for one batch) at step 100: 3.7206\n",
            "Training loss (for one batch) at step 200: 1.0152\n",
            "Training loss (for one batch) at step 300: 0.6709\n",
            "Validation acc: 0.651 precision: 0.762 recall: 0.501 f1_micro: 0.605 f1_macro: 0.381\n",
            "Time taken: 151.27s\n",
            "Epoch 2\n",
            "Training loss (for one batch) at step 0: 1.8463\n",
            "Training loss (for one batch) at step 100: 1.4258\n",
            "Training loss (for one batch) at step 200: 1.6938\n",
            "Training loss (for one batch) at step 300: 3.0405\n",
            "Validation acc: 0.677 precision: 0.767 recall: 0.610 f1_micro: 0.680 f1_macro: 0.566\n",
            "Time taken: 144.81s\n",
            "Epoch 3\n",
            "Training loss (for one batch) at step 0: 2.3851\n",
            "Training loss (for one batch) at step 100: 2.3934\n",
            "Training loss (for one batch) at step 200: 2.2866\n",
            "Training loss (for one batch) at step 300: 2.4842\n",
            "Validation acc: 0.769 precision: 0.776 recall: 0.764 f1_micro: 0.770 f1_macro: 0.761\n",
            "Time taken: 145.01s\n",
            "Epoch 4\n",
            "Training loss (for one batch) at step 0: 2.1692\n",
            "Training loss (for one batch) at step 100: 2.3040\n",
            "Training loss (for one batch) at step 200: 2.3996\n",
            "Training loss (for one batch) at step 300: 1.5079\n",
            "Validation acc: 0.759 precision: 0.764 recall: 0.758 f1_micro: 0.761 f1_macro: 0.752\n",
            "Time taken: 144.91s\n",
            "Epoch 5\n",
            "Training loss (for one batch) at step 0: 2.7622\n",
            "Training loss (for one batch) at step 100: 2.9931\n",
            "Training loss (for one batch) at step 200: 2.7247\n",
            "Training loss (for one batch) at step 300: 2.5213\n",
            "Validation acc: 0.749 precision: 0.756 recall: 0.746 f1_micro: 0.751 f1_macro: 0.751\n",
            "Time taken: 145.12s\n",
            "Epoch 6\n",
            "Training loss (for one batch) at step 0: 1.8776\n",
            "Training loss (for one batch) at step 100: 2.3351\n",
            "Training loss (for one batch) at step 200: 2.4669\n",
            "Training loss (for one batch) at step 300: 2.0233\n",
            "Validation acc: 0.760 precision: 0.771 recall: 0.754 f1_micro: 0.763 f1_macro: 0.756\n",
            "Time taken: 145.13s\n",
            "Epoch 7\n",
            "Training loss (for one batch) at step 0: 2.0723\n",
            "Training loss (for one batch) at step 100: 2.3058\n",
            "Training loss (for one batch) at step 200: 2.1257\n",
            "Training loss (for one batch) at step 300: 2.3303\n",
            "Validation acc: 0.767 precision: 0.770 recall: 0.765 f1_micro: 0.768 f1_macro: 0.759\n",
            "Time taken: 145.18s\n",
            "Restoring best weights relative to validation accuracy...\n"
          ]
        }
      ],
      "source": [
        "EPOCHS=8\n",
        "max_val_acc = 0.0\n",
        "best_weights = None\n",
        "\n",
        "l_iter = iter(labeled_ds)\n",
        "u_iter = iter(unlabeled_ds)\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"Epoch {epoch}\")\n",
        "  start_time = time.time()\n",
        "  for step in range(steps_per_epoch):\n",
        "    x, y = next(l_iter)\n",
        "    xu = next(u_iter)\n",
        "    \n",
        "    loss = train_step(x,y,xu)\n",
        "    if step % 100 == 0:\n",
        "      print(\n",
        "        \"Training loss (for one batch) at step %d: %.4f\"\n",
        "        % (step, float(loss))\n",
        "      )\n",
        "  for x_batch_val, y_batch_val in val_ds:\n",
        "      test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "  acc = float(val_acc_metric.result())\n",
        "  prec = float(val_prec_metric.result())\n",
        "  recall = float(val_recall_metric.result())\n",
        "  micro = float(f1_metric_micro.result())\n",
        "  macro = float(f1_metric_macro.result())\n",
        "\n",
        "  val_acc_metric.reset_states()\n",
        "  val_prec_metric.reset_states()\n",
        "  val_recall_metric.reset_states()\n",
        "  f1_metric_micro.reset_states()\n",
        "  f1_metric_macro.reset_states()\n",
        "\n",
        "  if acc > max_val_acc:\n",
        "    max_val_acc = acc\n",
        "    best_weights = model.get_weights()\n",
        "  print(f\"Validation acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")\n",
        "  print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "print(\"Restoring best weights relative to validation accuracy...\")\n",
        "model.set_weights(best_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCg9eBFXgpVt",
        "outputId": "10db87be-1eef-422e-989a-5277016ebf8d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test acc: 0.744 precision: 0.751 recall: 0.738 f1_micro: 0.745 f1_macro: 0.732\n"
          ]
        }
      ],
      "source": [
        "for x_batch_val, y_batch_val in test_ds:\n",
        "    test_step(x_batch_val, y_batch_val)\n",
        "acc = float(val_acc_metric.result())\n",
        "prec = float(val_prec_metric.result())\n",
        "recall = float(val_recall_metric.result())\n",
        "micro = float(f1_metric_micro.result())\n",
        "macro = float(f1_metric_macro.result())\n",
        "\n",
        "val_acc_metric.reset_states()\n",
        "val_prec_metric.reset_states()\n",
        "val_recall_metric.reset_states()\n",
        "f1_metric_micro.reset_states()\n",
        "f1_metric_macro.reset_states()\n",
        "print(f\"Test acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZZ1xeRz2gpVt",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model.save_weights(\"./checkpoints/contrastive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

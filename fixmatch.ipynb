{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IS_COLAB = True\n",
    "except:\n",
    "  IS_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "  !pip install tensorflow_addons\n",
    "  !pip install unidecode\n",
    "  !pip install transformers\n",
    "  !pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/SemiSupervised/*.csv ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 08:23:10.283929: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-31 08:23:10.802815: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64::/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:\n",
      "2023-03-31 08:23:10.802861: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64::/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:\n",
      "2023-03-31 08:23:10.802866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-31 08:23:11.436302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-31 08:23:11.436601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-31 08:23:11.440236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-31 08:23:11.440517: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-31 08:23:11.440801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-31 08:23:11.441066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[1], 'GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train_ner.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    s = unidecode(x)\n",
    "    s = str.lower(s)\n",
    "    s = re.sub(r\"\\[[a-z]+\\]\",\"\", s)\n",
    "    s = re.sub(r\"\\*\",\"\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9]+\",\" \",s)\n",
    "    s = re.sub(r\" +\",\" \",s)\n",
    "    s = re.sub(r\"(.)\\1+\",r\"\\1\",s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABUSE': 0, 'INSULT': 1, 'OTHER': 2, 'PROFANITY': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ids = {label_name:i for i, label_name in enumerate(sorted(set(data[\"label\"])))}\n",
    "label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32674</td>\n",
       "      <td>da de unde stii u mai [ORG] ca banii au fost p...</td>\n",
       "      <td>ABUSE</td>\n",
       "      <td>da de unde sti u mai ca bani au fost pt si nu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16514</td>\n",
       "      <td>m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>muie muie muie bai kakatule stai in banca ta d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32556</td>\n",
       "      <td>PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>pt ala care are treaba cu esti unul care nu ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23861</td>\n",
       "      <td>sunt bucuros ca [PERS] nu a mai venit la [ORG]...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>sunt bucuros ca nu a mai venit la jucator de d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21811</td>\n",
       "      <td>[PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>esti pe n es cu li bi l te asemeni cu de la c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text   label  \\\n",
       "0  32674  da de unde stii u mai [ORG] ca banii au fost p...   ABUSE   \n",
       "1  16514  m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...  INSULT   \n",
       "2  32556  PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...   OTHER   \n",
       "3  23861  sunt bucuros ca [PERS] nu a mai venit la [ORG]...   OTHER   \n",
       "4  21811  [PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...  INSULT   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  da de unde sti u mai ca bani au fost pt si nu ...  \n",
       "1  muie muie muie bai kakatule stai in banca ta d...  \n",
       "2  pt ala care are treaba cu esti unul care nu ar...  \n",
       "3  sunt bucuros ca nu a mai venit la jucator de d...  \n",
       "4   esti pe n es cu li bi l te asemeni cu de la c...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"preprocessed\"] = data[\"text\"].apply(preprocess)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32674</td>\n",
       "      <td>da de unde stii u mai [ORG] ca banii au fost p...</td>\n",
       "      <td>ABUSE</td>\n",
       "      <td>da de unde sti u mai ca bani au fost pt si nu ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16514</td>\n",
       "      <td>m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>muie muie muie bai kakatule stai in banca ta d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32556</td>\n",
       "      <td>PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>pt ala care are treaba cu esti unul care nu ar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23861</td>\n",
       "      <td>sunt bucuros ca [PERS] nu a mai venit la [ORG]...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>sunt bucuros ca nu a mai venit la jucator de d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21811</td>\n",
       "      <td>[PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>esti pe n es cu li bi l te asemeni cu de la c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text   label  \\\n",
       "0  32674  da de unde stii u mai [ORG] ca banii au fost p...   ABUSE   \n",
       "1  16514  m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...  INSULT   \n",
       "2  32556  PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...   OTHER   \n",
       "3  23861  sunt bucuros ca [PERS] nu a mai venit la [ORG]...   OTHER   \n",
       "4  21811  [PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...  INSULT   \n",
       "\n",
       "                                        preprocessed  class  \n",
       "0  da de unde sti u mai ca bani au fost pt si nu ...      0  \n",
       "1  muie muie muie bai kakatule stai in banca ta d...      1  \n",
       "2  pt ala care are treaba cu esti unul care nu ar...      2  \n",
       "3  sunt bucuros ca nu a mai venit la jucator de d...      2  \n",
       "4   esti pe n es cu li bi l te asemeni cu de la c...      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"class\"] = data[\"label\"].map(lambda x: label_ids[x])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OTHER        3649\n",
       "ABUSE        2768\n",
       "INSULT       2242\n",
       "PROFANITY    1294\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "tok_robert = AutoTokenizer.from_pretrained(\"readerbench/RoBERT-base\")\n",
    "robert = TFAutoModel.from_pretrained(\"readerbench/RoBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def split_ssl_data(ids_array,mask_array,labels,num_classes,label_percent):\n",
    "  labeled = None\n",
    "  unlabeled = None\n",
    "\n",
    "  for class_idx in range(num_classes):\n",
    "    class_ids = ids_array[labels==class_idx]\n",
    "    class_mask = mask_array[labels==class_idx]\n",
    "    sz = int(label_percent * class_ids.shape[0])\n",
    "\n",
    "    labels_reduced = labels[labels==class_idx][:sz]\n",
    "    labeled_ids, unlabeled_ids = class_ids[:sz], class_ids[sz:]\n",
    "    labeled_mask, unlabeled_mask = class_mask[:sz], class_mask[sz:]\n",
    "\n",
    "    if not labeled:\n",
    "      labeled = (labeled_ids, labeled_mask, labels_reduced)\n",
    "      unlabeled = (unlabeled_ids, unlabeled_mask)\n",
    "    else:\n",
    "      labeled = (\n",
    "          np.concatenate([labeled[0],labeled_ids]),\n",
    "          np.concatenate([labeled[1],labeled_mask]),\n",
    "          np.concatenate([labeled[2],labels_reduced])\n",
    "      )\n",
    "      unlabeled = (\n",
    "          np.concatenate([unlabeled[0],unlabeled_ids]),\n",
    "          np.concatenate([unlabeled[1],unlabeled_mask]),\n",
    "      )\n",
    "\n",
    "  return labeled, unlabeled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_robert(x):\n",
    "  t = tok_robert(x,padding=\"max_length\",max_length=96,truncation=True,return_tensors='np')\n",
    "  return t[\"input_ids\"], t[\"attention_mask\"]\n",
    "\n",
    "def map_func(input_ids, masks, labels):\n",
    "  return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "\n",
    "def map_func2(input_ids, masks):\n",
    "  return {'input_ids': input_ids, 'attention_mask': masks}\n",
    "\n",
    "def prepare_ds(filename,batch_size=64):\n",
    "  df = pd.read_csv(filename)\n",
    "  X_id_mask = df['text'].map(preprocess).apply(preprocess_robert).apply(pd.Series)\n",
    "\n",
    "  X_id_mask.columns = [\"input_ids\",\"attention_mask\"]\n",
    "\n",
    "  ids_array = np.squeeze(np.stack(X_id_mask.input_ids.values), axis=1)\n",
    "  mask_array = np.squeeze(np.stack(X_id_mask.attention_mask.values), axis=1)\n",
    "  labels = df[\"label\"].map(lambda x: label_ids[x]).values\n",
    "\n",
    "  res_ds = tf.data.Dataset.from_tensor_slices((ids_array, mask_array, labels)).map(map_func).shuffle(len(df)).batch(batch_size)\n",
    "  return res_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_train_ds(filename,batch_size=16,label_percent=0.05):\n",
    "  df = pd.read_csv(filename)\n",
    "  df = df.sample(frac=1)\n",
    "  X_id_mask = df['text'].map(preprocess).apply(preprocess_robert).apply(pd.Series)\n",
    "\n",
    "  X_id_mask.columns = [\"input_ids\",\"attention_mask\"]\n",
    "\n",
    "  ids_array = np.squeeze(np.stack(X_id_mask.input_ids.values), axis=1)\n",
    "  mask_array = np.squeeze(np.stack(X_id_mask.attention_mask.values), axis=1)\n",
    "  labels = df[\"label\"].map(lambda x: label_ids[x]).values\n",
    "\n",
    "  labeled, unlabeled = split_ssl_data(ids_array,mask_array,labels,len(label_ids),label_percent)\n",
    "  labeled_ds = tf.data.Dataset.from_tensor_slices(labeled)\n",
    "  labeled_ds = labeled_ds.map(map_func).shuffle(len(labeled_ds)).batch(batch_size).repeat()\n",
    "  unlabeled_ds = tf.data.Dataset.from_tensor_slices(unlabeled)\n",
    "  unlabeled_ds = unlabeled_ds.map(map_func2).shuffle(len(unlabeled_ds)).batch(batch_size).repeat()\n",
    "  \n",
    "  return labeled_ds, unlabeled_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "labeled_ds, unlabeled_ds = prepare_train_ds(\"train_ner.csv\")\n",
    "test_ds = prepare_ds(\"test_ner.csv\")\n",
    "val_ds = prepare_ds(\"validation_internal_ner.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class FixMatch(tf.keras.Model):\n",
    "  def __init__(self,bert_model,num_classes=4,**kwargs):\n",
    "    super(FixMatch,self).__init__(**kwargs)\n",
    "    self.bert = bert_model\n",
    "    # self.bert.trainable = False\n",
    "    self.num_classes = num_classes\n",
    "    self.weak_augment = tf.keras.layers.GaussianNoise(stddev=0.5)\n",
    "    self.strong_augment = tf.keras.layers.GaussianNoise(stddev=5)\n",
    "\n",
    "\n",
    "    # self.cls_head = tf.keras.Sequential([\n",
    "    #     tf.keras.layers.Bidirectional(\n",
    "    #         tf.keras.layers.LSTM(10),\n",
    "    #         merge_mode=\"ave\"\n",
    "    #     ),\n",
    "    #     tf.keras.layers.Dense(units=self.num_classes,activation=\"softmax\")\n",
    "    # ])\n",
    "    self.cls_head = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(256,activation=\"relu\"),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(self.num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    ids, mask = inputs\n",
    "    \n",
    "    embeds = self.bert(input_ids=ids, attention_mask=mask,training=training).pooler_output\n",
    "\n",
    "    strongs = self.strong_augment(embeds,training=training)\n",
    "    weaks = self.weak_augment(embeds,training=training)\n",
    "\n",
    "    strong_preds = self.cls_head(strongs,training=training)\n",
    "    weak_preds = self.cls_head(weaks,training=training)\n",
    "\n",
    "    return weak_preds, strong_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = FixMatch(bert_model=robert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "optim = tfa.optimizers.AdamW(weight_decay=0.001,learning_rate=0.005)\n",
    "optim2 = tfa.optimizers.AdamW(weight_decay=0.0,learning_rate=0.00001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_prec_metric = tf.keras.metrics.Precision(name=\"precision\")\n",
    "val_recall_metric = tf.keras.metrics.Recall(name=\"recall\")\n",
    "f1_metric_micro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='micro', name='f1_micro')\n",
    "f1_metric_macro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='macro', name='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "thresh = 0.9\n",
    "unsup_weight = 1.0\n",
    "steps_per_epoch = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x,y,xu):\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      wl,_ = model([x[\"input_ids\"],x[\"attention_mask\"]],training=True)\n",
    "      wu,su = model([xu[\"input_ids\"],xu[\"attention_mask\"]],training=True)\n",
    "      ls = loss_fn(y, wl)\n",
    "      mask = tf.reduce_max(wu,axis=1)>thresh\n",
    "      wu = wu[mask]\n",
    "      su = su[mask]\n",
    "      lu = loss_fn(tf.argmax(wu,axis=1),su)\n",
    "\n",
    "      loss = ls + unsup_weight * lu\n",
    "\n",
    "  grads = tape.gradient(loss, [model.cls_head.trainable_weights, model.bert.trainable_weights])\n",
    "  optim.apply_gradients(zip(grads[0], model.cls_head.trainable_weights))\n",
    "  optim2.apply_gradients(zip(grads[1], model.bert.trainable_weights))\n",
    "\n",
    "  return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    wpred,_ = model([x[\"input_ids\"],x[\"attention_mask\"]], training=False)\n",
    "    val_acc_metric.update_state(y, wpred)\n",
    "    true_hot = tf.one_hot(y, 4)\n",
    "    val_prec_metric.update_state(true_hot, wpred)\n",
    "    val_recall_metric.update_state(true_hot, wpred)\n",
    "    f1_metric_micro.update_state(true_hot, wpred)\n",
    "    f1_metric_macro.update_state(true_hot, wpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training loss (for one batch) at step 0: 1.3956\n",
      "Training loss (for one batch) at step 50: 0.9159\n",
      "Training loss (for one batch) at step 100: 0.5319\n",
      "Training loss (for one batch) at step 150: 0.7028\n",
      "Training loss (for one batch) at step 200: 0.4350\n",
      "Training loss (for one batch) at step 250: 0.4705\n",
      "Training loss (for one batch) at step 300: 0.6427\n",
      "Training loss (for one batch) at step 350: 0.8143\n",
      "Training loss (for one batch) at step 400: 0.6537\n",
      "Training loss (for one batch) at step 450: 0.6270\n",
      "Training loss (for one batch) at step 500: 0.6037\n",
      "Training loss (for one batch) at step 550: 0.5699\n",
      "Validation acc: 0.581 precision: 0.684 recall: 0.467 f1_micro: 0.555 f1_macro: 0.346\n",
      "Time taken: 230.34s\n",
      "Epoch 1\n",
      "Training loss (for one batch) at step 0: 0.4427\n",
      "Training loss (for one batch) at step 50: 1.0603\n",
      "Training loss (for one batch) at step 100: 0.3397\n",
      "Training loss (for one batch) at step 150: 2.5873\n",
      "Training loss (for one batch) at step 200: 1.9237\n",
      "Training loss (for one batch) at step 250: 0.3046\n",
      "Training loss (for one batch) at step 300: 0.3819\n",
      "Training loss (for one batch) at step 350: 0.9185\n",
      "Training loss (for one batch) at step 400: 0.3105\n",
      "Training loss (for one batch) at step 450: 0.3167\n",
      "Training loss (for one batch) at step 500: 0.3592\n",
      "Training loss (for one batch) at step 550: 0.4593\n",
      "Validation acc: 0.660 precision: 0.665 recall: 0.658 f1_micro: 0.662 f1_macro: 0.634\n",
      "Time taken: 214.50s\n",
      "Epoch 2\n",
      "Training loss (for one batch) at step 0: 0.5736\n",
      "Training loss (for one batch) at step 50: 0.2231\n",
      "Training loss (for one batch) at step 100: 0.4787\n",
      "Training loss (for one batch) at step 150: 0.1506\n",
      "Training loss (for one batch) at step 200: 0.3893\n",
      "Training loss (for one batch) at step 250: 0.0901\n",
      "Training loss (for one batch) at step 300: 0.2732\n",
      "Training loss (for one batch) at step 350: 0.6120\n",
      "Training loss (for one batch) at step 400: 0.1221\n",
      "Training loss (for one batch) at step 450: 0.4273\n",
      "Training loss (for one batch) at step 500: 0.2845\n",
      "Training loss (for one batch) at step 550: 0.4215\n",
      "Validation acc: 0.675 precision: 0.675 recall: 0.674 f1_micro: 0.674 f1_macro: 0.659\n",
      "Time taken: 215.46s\n",
      "Epoch 3\n",
      "Training loss (for one batch) at step 0: 0.1695\n",
      "Training loss (for one batch) at step 50: 0.0210\n",
      "Training loss (for one batch) at step 100: 0.0566\n",
      "Training loss (for one batch) at step 150: 0.1596\n",
      "Training loss (for one batch) at step 200: 0.1565\n",
      "Training loss (for one batch) at step 250: 0.7053\n",
      "Training loss (for one batch) at step 300: 0.1594\n",
      "Training loss (for one batch) at step 350: 0.0432\n",
      "Training loss (for one batch) at step 400: 0.0227\n",
      "Training loss (for one batch) at step 450: 0.1401\n",
      "Training loss (for one batch) at step 500: 0.0761\n",
      "Training loss (for one batch) at step 550: 0.7478\n",
      "Validation acc: 0.681 precision: 0.682 recall: 0.681 f1_micro: 0.681 f1_macro: 0.655\n",
      "Time taken: 215.59s\n",
      "Epoch 4\n",
      "Training loss (for one batch) at step 0: 0.1565\n",
      "Training loss (for one batch) at step 50: 0.1028\n",
      "Training loss (for one batch) at step 100: 0.6055\n",
      "Training loss (for one batch) at step 150: 0.4398\n",
      "Training loss (for one batch) at step 200: 0.2996\n",
      "Training loss (for one batch) at step 250: 0.1126\n",
      "Training loss (for one batch) at step 300: 0.2928\n",
      "Training loss (for one batch) at step 350: 0.0280\n",
      "Training loss (for one batch) at step 400: 0.0494\n",
      "Training loss (for one batch) at step 450: 0.1268\n",
      "Training loss (for one batch) at step 500: 0.2923\n",
      "Training loss (for one batch) at step 550: 0.0811\n",
      "Validation acc: 0.668 precision: 0.669 recall: 0.668 f1_micro: 0.669 f1_macro: 0.641\n",
      "Time taken: 215.38s\n",
      "Epoch 5\n",
      "Training loss (for one batch) at step 0: 0.0095\n",
      "Training loss (for one batch) at step 50: 0.8257\n",
      "Training loss (for one batch) at step 100: 0.0332\n",
      "Training loss (for one batch) at step 150: 0.0696\n",
      "Training loss (for one batch) at step 200: 0.0422\n",
      "Training loss (for one batch) at step 250: 0.0269\n",
      "Training loss (for one batch) at step 300: 0.1998\n",
      "Training loss (for one batch) at step 350: 0.0119\n",
      "Training loss (for one batch) at step 400: 0.2978\n",
      "Training loss (for one batch) at step 450: 0.4943\n",
      "Training loss (for one batch) at step 500: 0.5796\n",
      "Training loss (for one batch) at step 550: 0.0431\n",
      "Validation acc: 0.615 precision: 0.616 recall: 0.615 f1_micro: 0.615 f1_macro: 0.608\n",
      "Time taken: 215.40s\n",
      "Epoch 6\n",
      "Training loss (for one batch) at step 0: 1.0952\n",
      "Training loss (for one batch) at step 50: 0.4131\n",
      "Training loss (for one batch) at step 100: 0.1554\n",
      "Training loss (for one batch) at step 150: 0.0566\n",
      "Training loss (for one batch) at step 200: 0.1121\n",
      "Training loss (for one batch) at step 250: 0.4370\n",
      "Training loss (for one batch) at step 300: 0.0133\n",
      "Training loss (for one batch) at step 350: 0.2590\n",
      "Training loss (for one batch) at step 400: 0.0592\n",
      "Training loss (for one batch) at step 450: 0.7147\n",
      "Training loss (for one batch) at step 500: 0.0881\n",
      "Training loss (for one batch) at step 550: 0.0505\n",
      "Validation acc: 0.626 precision: 0.626 recall: 0.624 f1_micro: 0.625 f1_macro: 0.607\n",
      "Time taken: 215.39s\n",
      "Epoch 7\n",
      "Training loss (for one batch) at step 0: 0.0828\n",
      "Training loss (for one batch) at step 50: 1.6990\n",
      "Training loss (for one batch) at step 100: 0.0843\n",
      "Training loss (for one batch) at step 150: 0.0167\n",
      "Training loss (for one batch) at step 200: 0.0151\n",
      "Training loss (for one batch) at step 250: 0.0301\n",
      "Training loss (for one batch) at step 300: 0.0420\n",
      "Training loss (for one batch) at step 350: 0.5284\n",
      "Training loss (for one batch) at step 400: 0.1380\n",
      "Training loss (for one batch) at step 450: 0.0302\n",
      "Training loss (for one batch) at step 500: 0.0848\n",
      "Training loss (for one batch) at step 550: 0.7209\n",
      "Validation acc: 0.607 precision: 0.609 recall: 0.607 f1_micro: 0.608 f1_macro: 0.594\n",
      "Time taken: 215.39s\n",
      "Epoch 8\n",
      "Training loss (for one batch) at step 0: 0.0624\n",
      "Training loss (for one batch) at step 50: 0.1642\n",
      "Training loss (for one batch) at step 100: 0.1618\n",
      "Training loss (for one batch) at step 150: 0.0431\n",
      "Training loss (for one batch) at step 200: 0.0086\n",
      "Training loss (for one batch) at step 250: 0.0181\n",
      "Training loss (for one batch) at step 300: 0.0098\n",
      "Training loss (for one batch) at step 350: 0.3250\n",
      "Training loss (for one batch) at step 400: 0.2389\n",
      "Training loss (for one batch) at step 450: 0.1970\n",
      "Training loss (for one batch) at step 500: 0.0060\n",
      "Training loss (for one batch) at step 550: 0.0176\n",
      "Validation acc: 0.618 precision: 0.619 recall: 0.617 f1_micro: 0.618 f1_macro: 0.581\n",
      "Time taken: 215.36s\n",
      "Epoch 9\n",
      "Training loss (for one batch) at step 0: 0.8213\n",
      "Training loss (for one batch) at step 50: 0.2950\n",
      "Training loss (for one batch) at step 100: 0.1103\n",
      "Training loss (for one batch) at step 150: 0.1272\n",
      "Training loss (for one batch) at step 200: 0.0732\n",
      "Training loss (for one batch) at step 250: 0.0265\n",
      "Training loss (for one batch) at step 300: 0.7900\n",
      "Training loss (for one batch) at step 350: 0.3965\n",
      "Training loss (for one batch) at step 400: 0.0493\n",
      "Training loss (for one batch) at step 450: 0.0860\n",
      "Training loss (for one batch) at step 500: 0.0184\n",
      "Training loss (for one batch) at step 550: 0.2954\n",
      "Validation acc: 0.656 precision: 0.657 recall: 0.655 f1_micro: 0.656 f1_macro: 0.645\n",
      "Time taken: 215.21s\n",
      "Epoch 10\n",
      "Training loss (for one batch) at step 0: 0.0065\n",
      "Training loss (for one batch) at step 50: 0.0067\n",
      "Training loss (for one batch) at step 100: 0.0257\n",
      "Training loss (for one batch) at step 150: 0.1833\n",
      "Training loss (for one batch) at step 200: 0.0555\n",
      "Training loss (for one batch) at step 250: 0.2980\n",
      "Training loss (for one batch) at step 300: 0.0037\n",
      "Training loss (for one batch) at step 350: 0.0074\n",
      "Training loss (for one batch) at step 400: 0.0432\n",
      "Training loss (for one batch) at step 450: 0.0337\n",
      "Training loss (for one batch) at step 500: 0.0315\n",
      "Training loss (for one batch) at step 550: 0.0567\n",
      "Validation acc: 0.642 precision: 0.643 recall: 0.642 f1_micro: 0.643 f1_macro: 0.613\n",
      "Time taken: 215.35s\n",
      "Epoch 11\n",
      "Training loss (for one batch) at step 0: 0.0267\n",
      "Training loss (for one batch) at step 50: 0.1779\n",
      "Training loss (for one batch) at step 100: 0.6908\n",
      "Training loss (for one batch) at step 150: 0.3426\n",
      "Training loss (for one batch) at step 200: 0.3407\n",
      "Training loss (for one batch) at step 250: 0.1118\n",
      "Training loss (for one batch) at step 300: 0.0153\n",
      "Training loss (for one batch) at step 350: 0.1166\n",
      "Training loss (for one batch) at step 400: 0.0238\n",
      "Training loss (for one batch) at step 450: 0.0493\n",
      "Training loss (for one batch) at step 500: 0.0482\n",
      "Training loss (for one batch) at step 550: 0.0045\n",
      "Validation acc: 0.631 precision: 0.631 recall: 0.631 f1_micro: 0.631 f1_macro: 0.629\n",
      "Time taken: 216.18s\n",
      "Epoch 12\n",
      "Training loss (for one batch) at step 0: 0.2985\n",
      "Training loss (for one batch) at step 50: 0.0155\n",
      "Training loss (for one batch) at step 100: 0.0885\n",
      "Training loss (for one batch) at step 150: 0.0009\n",
      "Training loss (for one batch) at step 200: 0.0390\n",
      "Training loss (for one batch) at step 250: 0.0018\n",
      "Training loss (for one batch) at step 300: 0.0132\n",
      "Training loss (for one batch) at step 350: 0.0143\n",
      "Training loss (for one batch) at step 400: 0.0018\n",
      "Training loss (for one batch) at step 450: 0.2422\n",
      "Training loss (for one batch) at step 500: 0.0501\n",
      "Training loss (for one batch) at step 550: 0.3650\n",
      "Validation acc: 0.619 precision: 0.619 recall: 0.619 f1_micro: 0.619 f1_macro: 0.608\n",
      "Time taken: 216.50s\n",
      "Epoch 13\n",
      "Training loss (for one batch) at step 0: 0.0072\n",
      "Training loss (for one batch) at step 50: 0.0029\n",
      "Training loss (for one batch) at step 100: 0.0342\n",
      "Training loss (for one batch) at step 150: 0.0078\n",
      "Training loss (for one batch) at step 200: 0.1752\n",
      "Training loss (for one batch) at step 250: 0.0125\n",
      "Training loss (for one batch) at step 300: 0.0133\n",
      "Training loss (for one batch) at step 350: 0.0097\n",
      "Training loss (for one batch) at step 400: 0.2810\n",
      "Training loss (for one batch) at step 450: 0.0046\n",
      "Training loss (for one batch) at step 500: 0.5900\n",
      "Training loss (for one batch) at step 550: 0.3120\n",
      "Validation acc: 0.644 precision: 0.644 recall: 0.642 f1_micro: 0.643 f1_macro: 0.643\n",
      "Time taken: 216.32s\n",
      "Epoch 14\n",
      "Training loss (for one batch) at step 0: 0.2661\n",
      "Training loss (for one batch) at step 50: 0.0126\n",
      "Training loss (for one batch) at step 100: 0.3312\n",
      "Training loss (for one batch) at step 150: 0.3838\n",
      "Training loss (for one batch) at step 200: 0.0542\n",
      "Training loss (for one batch) at step 250: 0.0397\n",
      "Training loss (for one batch) at step 300: 0.0739\n",
      "Training loss (for one batch) at step 350: 0.0809\n",
      "Training loss (for one batch) at step 400: 0.1680\n",
      "Training loss (for one batch) at step 450: 0.0111\n",
      "Training loss (for one batch) at step 500: 0.5097\n",
      "Training loss (for one batch) at step 550: 0.0197\n",
      "Validation acc: 0.621 precision: 0.624 recall: 0.620 f1_micro: 0.622 f1_macro: 0.599\n",
      "Time taken: 216.30s\n",
      "Epoch 15\n",
      "Training loss (for one batch) at step 0: 0.0162\n",
      "Training loss (for one batch) at step 50: 0.0313\n",
      "Training loss (for one batch) at step 100: 0.4966\n",
      "Training loss (for one batch) at step 150: 0.3845\n",
      "Training loss (for one batch) at step 200: 0.1386\n",
      "Training loss (for one batch) at step 250: 0.0397\n",
      "Training loss (for one batch) at step 300: 0.0225\n",
      "Training loss (for one batch) at step 350: 0.0019\n",
      "Training loss (for one batch) at step 400: 0.0087\n",
      "Training loss (for one batch) at step 450: 0.0115\n",
      "Training loss (for one batch) at step 500: 0.0226\n",
      "Training loss (for one batch) at step 550: 0.0254\n",
      "Validation acc: 0.638 precision: 0.640 recall: 0.638 f1_micro: 0.639 f1_macro: 0.610\n",
      "Time taken: 216.36s\n",
      "Restoring best weights relative to validation accuracy...\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=16\n",
    "max_val_acc = 0.0\n",
    "best_weights = None\n",
    "\n",
    "l_iter = iter(labeled_ds)\n",
    "u_iter = iter(unlabeled_ds)\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f\"Epoch {epoch}\")\n",
    "  start_time = time.time()\n",
    "  for step in range(steps_per_epoch):\n",
    "    x, y = next(l_iter)\n",
    "    xu = next(u_iter)\n",
    "    \n",
    "    loss = train_step(x,y,xu)\n",
    "    if step % 50 == 0:\n",
    "      print(\n",
    "        \"Training loss (for one batch) at step %d: %.4f\"\n",
    "        % (step, float(loss))\n",
    "      )\n",
    "  for x_batch_val, y_batch_val in val_ds:\n",
    "      test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "  acc = float(val_acc_metric.result())\n",
    "  prec = float(val_prec_metric.result())\n",
    "  recall = float(val_recall_metric.result())\n",
    "  micro = float(f1_metric_micro.result())\n",
    "  macro = float(f1_metric_macro.result())\n",
    "\n",
    "  val_acc_metric.reset_states()\n",
    "  val_prec_metric.reset_states()\n",
    "  val_recall_metric.reset_states()\n",
    "  f1_metric_micro.reset_states()\n",
    "  f1_metric_macro.reset_states()\n",
    "\n",
    "  if acc > max_val_acc:\n",
    "    max_val_acc = acc\n",
    "    best_weights = model.get_weights()\n",
    "  print(f\"Validation acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")\n",
    "  print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "print(\"Restoring best weights relative to validation accuracy...\")\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.687 precision: 0.688 recall: 0.687 f1_micro: 0.688 f1_macro: 0.667\n"
     ]
    }
   ],
   "source": [
    "for x_batch_val, y_batch_val in test_ds:\n",
    "    test_step(x_batch_val, y_batch_val)\n",
    "acc = float(val_acc_metric.result())\n",
    "prec = float(val_prec_metric.result())\n",
    "recall = float(val_recall_metric.result())\n",
    "micro = float(f1_metric_micro.result())\n",
    "macro = float(f1_metric_macro.result())\n",
    "\n",
    "val_acc_metric.reset_states()\n",
    "val_prec_metric.reset_states()\n",
    "val_recall_metric.reset_states()\n",
    "f1_metric_micro.reset_states()\n",
    "f1_metric_macro.reset_states()\n",
    "print(f\"Test acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IS_COLAB = True\n",
    "except:\n",
    "  IS_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "  !pip install tensorflow_addons\n",
    "  !pip install unidecode\n",
    "  !pip install transformers\n",
    "  !pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/SemiSupervised/*.csv ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train_ner.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    s = unidecode(x)\n",
    "    s = str.lower(s)\n",
    "    s = re.sub(r\"\\[[a-z]+\\]\",\"\", s)\n",
    "    s = re.sub(r\"\\*\",\"\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9]+\",\" \",s)\n",
    "    s = re.sub(r\" +\",\" \",s)\n",
    "    s = re.sub(r\"(.)\\1+\",r\"\\1\",s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABUSE': 0, 'INSULT': 1, 'OTHER': 2, 'PROFANITY': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_ids = {label_name:i for i, label_name in enumerate(sorted(set(data[\"label\"])))}\n",
    "label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32674</td>\n",
       "      <td>da de unde stii u mai [ORG] ca banii au fost p...</td>\n",
       "      <td>ABUSE</td>\n",
       "      <td>da de unde sti u mai ca bani au fost pt si nu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16514</td>\n",
       "      <td>m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>muie muie muie bai kakatule stai in banca ta d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32556</td>\n",
       "      <td>PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>pt ala care are treaba cu esti unul care nu ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23861</td>\n",
       "      <td>sunt bucuros ca [PERS] nu a mai venit la [ORG]...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>sunt bucuros ca nu a mai venit la jucator de d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21811</td>\n",
       "      <td>[PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>esti pe n es cu li bi l te asemeni cu de la c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text   label  \\\n",
       "0  32674  da de unde stii u mai [ORG] ca banii au fost p...   ABUSE   \n",
       "1  16514  m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...  INSULT   \n",
       "2  32556  PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...   OTHER   \n",
       "3  23861  sunt bucuros ca [PERS] nu a mai venit la [ORG]...   OTHER   \n",
       "4  21811  [PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...  INSULT   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  da de unde sti u mai ca bani au fost pt si nu ...  \n",
       "1  muie muie muie bai kakatule stai in banca ta d...  \n",
       "2  pt ala care are treaba cu esti unul care nu ar...  \n",
       "3  sunt bucuros ca nu a mai venit la jucator de d...  \n",
       "4   esti pe n es cu li bi l te asemeni cu de la c...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"preprocessed\"] = data[\"text\"].apply(preprocess)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32674</td>\n",
       "      <td>da de unde stii u mai [ORG] ca banii au fost p...</td>\n",
       "      <td>ABUSE</td>\n",
       "      <td>da de unde sti u mai ca bani au fost pt si nu ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16514</td>\n",
       "      <td>m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>muie muie muie bai kakatule stai in banca ta d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32556</td>\n",
       "      <td>PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>pt ala care are treaba cu esti unul care nu ar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23861</td>\n",
       "      <td>sunt bucuros ca [PERS] nu a mai venit la [ORG]...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>sunt bucuros ca nu a mai venit la jucator de d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21811</td>\n",
       "      <td>[PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>esti pe n es cu li bi l te asemeni cu de la c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text   label  \\\n",
       "0  32674  da de unde stii u mai [ORG] ca banii au fost p...   ABUSE   \n",
       "1  16514  m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...  INSULT   \n",
       "2  32556  PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...   OTHER   \n",
       "3  23861  sunt bucuros ca [PERS] nu a mai venit la [ORG]...   OTHER   \n",
       "4  21811  [PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...  INSULT   \n",
       "\n",
       "                                        preprocessed  class  \n",
       "0  da de unde sti u mai ca bani au fost pt si nu ...      0  \n",
       "1  muie muie muie bai kakatule stai in banca ta d...      1  \n",
       "2  pt ala care are treaba cu esti unul care nu ar...      2  \n",
       "3  sunt bucuros ca nu a mai venit la jucator de d...      2  \n",
       "4   esti pe n es cu li bi l te asemeni cu de la c...      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"class\"] = data[\"label\"].map(lambda x: label_ids[x])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OTHER        3649\n",
       "ABUSE        2768\n",
       "INSULT       2242\n",
       "PROFANITY    1294\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_robert = AutoTokenizer.from_pretrained(\"readerbench/RoBERT-base\")\n",
    "robert = TFAutoModel.from_pretrained(\"readerbench/RoBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ssl_data(ids_array,mask_array,labels,num_classes,label_percent):\n",
    "  labeled = None\n",
    "  unlabeled = None\n",
    "\n",
    "  for class_idx in range(num_classes):\n",
    "    class_ids = ids_array[labels==class_idx]\n",
    "    class_mask = mask_array[labels==class_idx]\n",
    "    sz = int(label_percent * class_ids.shape[0])\n",
    "\n",
    "    labels_reduced = labels[labels==class_idx][:sz]\n",
    "    labeled_ids, unlabeled_ids = class_ids[:sz], class_ids[sz:]\n",
    "    labeled_mask, unlabeled_mask = class_mask[:sz], class_mask[sz:]\n",
    "\n",
    "    if not labeled:\n",
    "      labeled = (labeled_ids, labeled_mask, labels_reduced)\n",
    "      unlabeled = (unlabeled_ids, unlabeled_mask)\n",
    "    else:\n",
    "      labeled = (\n",
    "          np.concatenate([labeled[0],labeled_ids]),\n",
    "          np.concatenate([labeled[1],labeled_mask]),\n",
    "          np.concatenate([labeled[2],labels_reduced])\n",
    "      )\n",
    "      unlabeled = (\n",
    "          np.concatenate([unlabeled[0],unlabeled_ids]),\n",
    "          np.concatenate([unlabeled[1],unlabeled_mask]),\n",
    "      )\n",
    "\n",
    "  return labeled, unlabeled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_robert(x):\n",
    "  t = tok_robert(x,padding=\"max_length\",max_length=96,truncation=True,return_tensors='np')\n",
    "  return t[\"input_ids\"], t[\"attention_mask\"]\n",
    "\n",
    "def map_func(input_ids, masks, labels):\n",
    "  return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "\n",
    "def map_func2(input_ids, masks):\n",
    "  return {'input_ids': input_ids, 'attention_mask': masks}\n",
    "\n",
    "def prepare_ds(filename,batch_size=64,label_percent=0.8):\n",
    "  df = pd.read_csv(filename)\n",
    "  X_id_mask = df['text'].map(preprocess).apply(preprocess_robert).apply(pd.Series)\n",
    "\n",
    "  X_id_mask.columns = [\"input_ids\",\"attention_mask\"]\n",
    "\n",
    "  ids_array = np.squeeze(np.stack(X_id_mask.input_ids.values), axis=1)\n",
    "  mask_array = np.squeeze(np.stack(X_id_mask.attention_mask.values), axis=1)\n",
    "  labels = df[\"label\"].map(lambda x: label_ids[x]).values\n",
    "\n",
    "  res_ds = tf.data.Dataset.from_tensor_slices((ids_array, mask_array, labels)).map(map_func).shuffle(len(df)).batch(batch_size)\n",
    "  return res_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_ds(filename,batch_size=16,label_percent=0.05):\n",
    "  df = pd.read_csv(filename)\n",
    "  X_id_mask = df['text'].map(preprocess).apply(preprocess_robert).apply(pd.Series)\n",
    "\n",
    "  X_id_mask.columns = [\"input_ids\",\"attention_mask\"]\n",
    "\n",
    "  ids_array = np.squeeze(np.stack(X_id_mask.input_ids.values), axis=1)\n",
    "  mask_array = np.squeeze(np.stack(X_id_mask.attention_mask.values), axis=1)\n",
    "  labels = df[\"label\"].map(lambda x: label_ids[x]).values\n",
    "\n",
    "  labeled, unlabeled = split_ssl_data(ids_array,mask_array,labels,len(label_ids),label_percent)\n",
    "  labeled_ds = tf.data.Dataset.from_tensor_slices(labeled)\n",
    "  labeled_ds = labeled_ds.map(map_func).shuffle(len(labeled_ds)).batch(batch_size).repeat()\n",
    "  unlabeled_ds = tf.data.Dataset.from_tensor_slices(unlabeled)\n",
    "  unlabeled_ds = unlabeled_ds.map(map_func2).shuffle(len(unlabeled_ds)).batch(4*batch_size).repeat()\n",
    "  \n",
    "  return labeled_ds, unlabeled_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ds, unlabeled_ds = prepare_train_ds(\"train_ner.csv\")\n",
    "test_ds = prepare_ds(\"test_ner.csv\")\n",
    "val_ds = prepare_ds(\"validation_internal_ner.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixMatch(tf.keras.Model):\n",
    "  def __init__(self,bert_model,num_classes=4,**kwargs):\n",
    "    super(FixMatch,self).__init__(**kwargs)\n",
    "    self.bert = bert_model\n",
    "    self.bert.trainable = False\n",
    "    self.num_classes = num_classes\n",
    "    self.weak_augment = tf.keras.layers.GaussianNoise(stddev=0.5)\n",
    "    self.strong_augment = tf.keras.layers.GaussianNoise(stddev=5)\n",
    "\n",
    "\n",
    "    self.cls_head = tf.keras.Sequential([\n",
    "        tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(10),\n",
    "            merge_mode=\"ave\"\n",
    "        ),\n",
    "        tf.keras.layers.Dense(units=self.num_classes,activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    ids, mask = inputs\n",
    "    \n",
    "    embeds = self.bert(input_ids=ids, attention_mask=mask).last_hidden_state\n",
    "\n",
    "    strongs = self.strong_augment(embeds,training=training)\n",
    "    weaks = self.weak_augment(embeds,training=training)\n",
    "\n",
    "    strong_preds = self.cls_head(strongs)\n",
    "    weak_preds = self.cls_head(weaks)\n",
    "\n",
    "    return weak_preds, strong_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FixMatch(bert_model=robert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tfa.optimizers.AdamW(weight_decay=0.001,learning_rate=0.005)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_prec_metric = tf.keras.metrics.Precision(name=\"precision\")\n",
    "val_recall_metric = tf.keras.metrics.Recall(name=\"recall\")\n",
    "f1_metric_micro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='micro', name='f1_micro')\n",
    "f1_metric_macro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='macro', name='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.9\n",
    "unsup_weight = 1.0\n",
    "steps_per_epoch = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x,y,xu):\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      wl,_ = model([x[\"input_ids\"],x[\"attention_mask\"]],training=True)\n",
    "      wu,su = model([xu[\"input_ids\"],xu[\"attention_mask\"]],training=True)\n",
    "      ls = loss_fn(y, wl)\n",
    "      mask = tf.reduce_max(wu,axis=1)>thresh\n",
    "      wu = wu[mask]\n",
    "      su = su[mask]\n",
    "      lu = loss_fn(tf.argmax(wu,axis=1),su)\n",
    "\n",
    "      loss = ls + unsup_weight * lu\n",
    "\n",
    "  grads = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "  optim.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "  train_acc_metric.update_state(y, wl)\n",
    "\n",
    "  return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    wpred,_ = model([x[\"input_ids\"],x[\"attention_mask\"]], training=False)\n",
    "    val_acc_metric.update_state(y, wpred)\n",
    "    true_hot = tf.one_hot(y, 4)\n",
    "    val_prec_metric.update_state(true_hot, wpred)\n",
    "    val_recall_metric.update_state(true_hot, wpred)\n",
    "    f1_metric_micro.update_state(true_hot, wpred)\n",
    "    f1_metric_macro.update_state(true_hot, wpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training loss (for one batch) at step 0: 1.4448\n",
      "Training loss (for one batch) at step 50: 1.1548\n",
      "Training loss (for one batch) at step 100: 1.1461\n",
      "Validation acc: 0.507 precision: 0.651 recall: 0.319 f1_micro: 0.429 f1_macro: 0.269\n",
      "Time taken: 72.94s\n",
      "Epoch 1\n",
      "Training loss (for one batch) at step 0: 1.0179\n",
      "Training loss (for one batch) at step 50: 0.9364\n",
      "Training loss (for one batch) at step 100: 0.6395\n",
      "Validation acc: 0.526 precision: 0.596 recall: 0.411 f1_micro: 0.487 f1_macro: 0.354\n",
      "Time taken: 52.04s\n",
      "Epoch 2\n",
      "Training loss (for one batch) at step 0: 0.7625\n",
      "Training loss (for one batch) at step 50: 0.5597\n",
      "Training loss (for one batch) at step 100: 2.0320\n",
      "Validation acc: 0.522 precision: 0.586 recall: 0.430 f1_micro: 0.496 f1_macro: 0.381\n",
      "Time taken: 52.07s\n",
      "Epoch 3\n",
      "Training loss (for one batch) at step 0: 1.2739\n",
      "Training loss (for one batch) at step 50: 1.1842\n",
      "Training loss (for one batch) at step 100: 1.1210\n",
      "Validation acc: 0.528 precision: 0.606 recall: 0.419 f1_micro: 0.495 f1_macro: 0.410\n",
      "Time taken: 52.34s\n",
      "Epoch 4\n",
      "Training loss (for one batch) at step 0: 1.0104\n",
      "Training loss (for one batch) at step 50: 1.0056\n",
      "Training loss (for one batch) at step 100: 0.7384\n",
      "Validation acc: 0.512 precision: 0.571 recall: 0.406 f1_micro: 0.474 f1_macro: 0.421\n",
      "Time taken: 52.29s\n",
      "Epoch 5\n",
      "Training loss (for one batch) at step 0: 0.7637\n",
      "Training loss (for one batch) at step 50: 0.6732\n",
      "Training loss (for one batch) at step 100: 0.5491\n",
      "Validation acc: 0.494 precision: 0.552 recall: 0.404 f1_micro: 0.466 f1_macro: 0.429\n",
      "Time taken: 52.37s\n",
      "Epoch 6\n",
      "Training loss (for one batch) at step 0: 0.5236\n",
      "Training loss (for one batch) at step 50: 0.5720\n",
      "Training loss (for one batch) at step 100: 0.4918\n",
      "Validation acc: 0.510 precision: 0.571 recall: 0.432 f1_micro: 0.492 f1_macro: 0.443\n",
      "Time taken: 52.32s\n",
      "Epoch 7\n",
      "Training loss (for one batch) at step 0: 0.4278\n",
      "Training loss (for one batch) at step 50: 0.3912\n",
      "Training loss (for one batch) at step 100: 0.5246\n",
      "Validation acc: 0.527 precision: 0.591 recall: 0.434 f1_micro: 0.501 f1_macro: 0.414\n",
      "Time taken: 52.45s\n",
      "Epoch 8\n",
      "Training loss (for one batch) at step 0: 0.5826\n",
      "Training loss (for one batch) at step 50: 0.2795\n",
      "Training loss (for one batch) at step 100: 0.4822\n",
      "Validation acc: 0.537 precision: 0.586 recall: 0.482 f1_micro: 0.529 f1_macro: 0.415\n",
      "Time taken: 52.46s\n",
      "Epoch 9\n",
      "Training loss (for one batch) at step 0: 0.7227\n",
      "Training loss (for one batch) at step 50: 0.3805\n",
      "Training loss (for one batch) at step 100: 0.3730\n",
      "Validation acc: 0.514 precision: 0.574 recall: 0.433 f1_micro: 0.493 f1_macro: 0.446\n",
      "Time taken: 52.36s\n",
      "Epoch 10\n",
      "Training loss (for one batch) at step 0: 0.4253\n",
      "Training loss (for one batch) at step 50: 0.7828\n",
      "Training loss (for one batch) at step 100: 0.3586\n",
      "Validation acc: 0.516 precision: 0.576 recall: 0.452 f1_micro: 0.506 f1_macro: 0.426\n",
      "Time taken: 52.46s\n",
      "Epoch 11\n",
      "Training loss (for one batch) at step 0: 0.4286\n",
      "Training loss (for one batch) at step 50: 0.5332\n",
      "Training loss (for one batch) at step 100: 0.9501\n",
      "Validation acc: 0.517 precision: 0.558 recall: 0.452 f1_micro: 0.500 f1_macro: 0.448\n",
      "Time taken: 52.34s\n",
      "Epoch 12\n",
      "Training loss (for one batch) at step 0: 0.5446\n",
      "Training loss (for one batch) at step 50: 0.9241\n",
      "Training loss (for one batch) at step 100: 0.6651\n",
      "Validation acc: 0.506 precision: 0.565 recall: 0.422 f1_micro: 0.483 f1_macro: 0.429\n",
      "Time taken: 52.40s\n",
      "Epoch 13\n",
      "Training loss (for one batch) at step 0: 0.8750\n",
      "Training loss (for one batch) at step 50: 0.6925\n",
      "Training loss (for one batch) at step 100: 0.5513\n",
      "Validation acc: 0.524 precision: 0.572 recall: 0.475 f1_micro: 0.519 f1_macro: 0.452\n",
      "Time taken: 52.44s\n",
      "Epoch 14\n",
      "Training loss (for one batch) at step 0: 0.8033\n",
      "Training loss (for one batch) at step 50: 0.7705\n",
      "Training loss (for one batch) at step 100: 0.7706\n",
      "Validation acc: 0.524 precision: 0.558 recall: 0.468 f1_micro: 0.509 f1_macro: 0.456\n",
      "Time taken: 52.44s\n",
      "Epoch 15\n",
      "Training loss (for one batch) at step 0: 0.2912\n",
      "Training loss (for one batch) at step 50: 0.6629\n",
      "Training loss (for one batch) at step 100: 1.1632\n",
      "Validation acc: 0.536 precision: 0.571 recall: 0.474 f1_micro: 0.518 f1_macro: 0.420\n",
      "Time taken: 52.53s\n",
      "Epoch 16\n",
      "Training loss (for one batch) at step 0: 0.4788\n",
      "Training loss (for one batch) at step 50: 0.7050\n",
      "Training loss (for one batch) at step 100: 1.0977\n",
      "Validation acc: 0.510 precision: 0.550 recall: 0.439 f1_micro: 0.488 f1_macro: 0.452\n",
      "Time taken: 52.45s\n",
      "Epoch 17\n",
      "Training loss (for one batch) at step 0: 1.1263\n",
      "Training loss (for one batch) at step 50: 1.5138\n",
      "Training loss (for one batch) at step 100: 0.7218\n",
      "Validation acc: 0.536 precision: 0.576 recall: 0.470 f1_micro: 0.518 f1_macro: 0.401\n",
      "Time taken: 52.40s\n",
      "Epoch 18\n",
      "Training loss (for one batch) at step 0: 0.9216\n",
      "Training loss (for one batch) at step 50: 0.6485\n",
      "Training loss (for one batch) at step 100: 0.8870\n",
      "Validation acc: 0.526 precision: 0.564 recall: 0.471 f1_micro: 0.513 f1_macro: 0.441\n",
      "Time taken: 52.54s\n",
      "Epoch 19\n",
      "Training loss (for one batch) at step 0: 0.6553\n",
      "Training loss (for one batch) at step 50: 0.5719\n",
      "Training loss (for one batch) at step 100: 0.6241\n",
      "Validation acc: 0.531 precision: 0.582 recall: 0.459 f1_micro: 0.513 f1_macro: 0.432\n",
      "Time taken: 52.50s\n",
      "Epoch 20\n",
      "Training loss (for one batch) at step 0: 0.5106\n",
      "Training loss (for one batch) at step 50: 0.6339\n",
      "Training loss (for one batch) at step 100: 0.5800\n",
      "Validation acc: 0.514 precision: 0.568 recall: 0.448 f1_micro: 0.501 f1_macro: 0.449\n",
      "Time taken: 52.49s\n",
      "Epoch 21\n",
      "Training loss (for one batch) at step 0: 0.9376\n",
      "Training loss (for one batch) at step 50: 0.8120\n",
      "Training loss (for one batch) at step 100: 0.8562\n",
      "Validation acc: 0.515 precision: 0.555 recall: 0.453 f1_micro: 0.499 f1_macro: 0.450\n",
      "Time taken: 52.34s\n",
      "Epoch 22\n",
      "Training loss (for one batch) at step 0: 0.4482\n",
      "Training loss (for one batch) at step 50: 0.8230\n",
      "Training loss (for one batch) at step 100: 1.0124\n",
      "Validation acc: 0.519 precision: 0.560 recall: 0.459 f1_micro: 0.504 f1_macro: 0.429\n",
      "Time taken: 52.50s\n",
      "Epoch 23\n",
      "Training loss (for one batch) at step 0: 0.7586\n",
      "Training loss (for one batch) at step 50: 0.7711\n",
      "Training loss (for one batch) at step 100: 0.8814\n",
      "Validation acc: 0.515 precision: 0.561 recall: 0.446 f1_micro: 0.497 f1_macro: 0.454\n",
      "Time taken: 52.44s\n",
      "Epoch 24\n",
      "Training loss (for one batch) at step 0: 0.5069\n",
      "Training loss (for one batch) at step 50: 0.7146\n",
      "Training loss (for one batch) at step 100: 1.1718\n",
      "Validation acc: 0.528 precision: 0.579 recall: 0.464 f1_micro: 0.515 f1_macro: 0.455\n",
      "Time taken: 52.57s\n",
      "Epoch 25\n",
      "Training loss (for one batch) at step 0: 0.5590\n",
      "Training loss (for one batch) at step 50: 0.2701\n",
      "Training loss (for one batch) at step 100: 0.5534\n",
      "Validation acc: 0.524 precision: 0.573 recall: 0.472 f1_micro: 0.517 f1_macro: 0.460\n",
      "Time taken: 52.50s\n",
      "Epoch 26\n",
      "Training loss (for one batch) at step 0: 0.5788\n",
      "Training loss (for one batch) at step 50: 0.3283\n",
      "Training loss (for one batch) at step 100: 0.9092\n",
      "Validation acc: 0.494 precision: 0.539 recall: 0.433 f1_micro: 0.480 f1_macro: 0.437\n",
      "Time taken: 52.48s\n",
      "Epoch 27\n",
      "Training loss (for one batch) at step 0: 0.5178\n",
      "Training loss (for one batch) at step 50: 0.7526\n",
      "Training loss (for one batch) at step 100: 0.4805\n",
      "Validation acc: 0.537 precision: 0.569 recall: 0.476 f1_micro: 0.518 f1_macro: 0.454\n",
      "Time taken: 52.53s\n",
      "Epoch 28\n",
      "Training loss (for one batch) at step 0: 0.4321\n",
      "Training loss (for one batch) at step 50: 0.5583\n",
      "Training loss (for one batch) at step 100: 0.7333\n",
      "Validation acc: 0.537 precision: 0.580 recall: 0.477 f1_micro: 0.523 f1_macro: 0.451\n",
      "Time taken: 52.53s\n",
      "Epoch 29\n",
      "Training loss (for one batch) at step 0: 0.8608\n",
      "Training loss (for one batch) at step 50: 0.4551\n",
      "Training loss (for one batch) at step 100: 0.4471\n",
      "Validation acc: 0.521 precision: 0.564 recall: 0.463 f1_micro: 0.509 f1_macro: 0.436\n",
      "Time taken: 52.47s\n",
      "Epoch 30\n",
      "Training loss (for one batch) at step 0: 0.8668\n",
      "Training loss (for one batch) at step 50: 0.7914\n",
      "Training loss (for one batch) at step 100: 0.6922\n",
      "Validation acc: 0.508 precision: 0.559 recall: 0.429 f1_micro: 0.485 f1_macro: 0.435\n",
      "Time taken: 52.53s\n",
      "Epoch 31\n",
      "Training loss (for one batch) at step 0: 0.8348\n",
      "Training loss (for one batch) at step 50: 0.5003\n",
      "Training loss (for one batch) at step 100: 0.3031\n",
      "Validation acc: 0.509 precision: 0.550 recall: 0.441 f1_micro: 0.490 f1_macro: 0.433\n",
      "Time taken: 52.55s\n",
      "Epoch 32\n",
      "Training loss (for one batch) at step 0: 0.4804\n",
      "Training loss (for one batch) at step 50: 0.8495\n",
      "Training loss (for one batch) at step 100: 1.1158\n",
      "Validation acc: 0.503 precision: 0.538 recall: 0.456 f1_micro: 0.493 f1_macro: 0.454\n",
      "Time taken: 52.54s\n",
      "Epoch 33\n",
      "Training loss (for one batch) at step 0: 0.3204\n",
      "Training loss (for one batch) at step 50: 0.5784\n",
      "Training loss (for one batch) at step 100: 0.7918\n",
      "Validation acc: 0.496 precision: 0.539 recall: 0.423 f1_micro: 0.474 f1_macro: 0.443\n",
      "Time taken: 52.59s\n",
      "Epoch 34\n",
      "Training loss (for one batch) at step 0: 0.4507\n",
      "Training loss (for one batch) at step 50: 0.5669\n",
      "Training loss (for one batch) at step 100: 0.8452\n",
      "Validation acc: 0.496 precision: 0.545 recall: 0.418 f1_micro: 0.473 f1_macro: 0.446\n",
      "Time taken: 52.45s\n",
      "Epoch 35\n",
      "Training loss (for one batch) at step 0: 0.6411\n",
      "Training loss (for one batch) at step 50: 0.7982\n",
      "Training loss (for one batch) at step 100: 0.4642\n",
      "Validation acc: 0.545 precision: 0.577 recall: 0.498 f1_micro: 0.535 f1_macro: 0.436\n",
      "Time taken: 52.47s\n",
      "Epoch 36\n",
      "Training loss (for one batch) at step 0: 0.6864\n",
      "Training loss (for one batch) at step 50: 0.6060\n",
      "Training loss (for one batch) at step 100: 0.7118\n",
      "Validation acc: 0.522 precision: 0.558 recall: 0.465 f1_micro: 0.507 f1_macro: 0.436\n",
      "Time taken: 52.51s\n",
      "Epoch 37\n",
      "Training loss (for one batch) at step 0: 0.7891\n",
      "Training loss (for one batch) at step 50: 0.4952\n",
      "Training loss (for one batch) at step 100: 0.8019\n",
      "Validation acc: 0.481 precision: 0.518 recall: 0.409 f1_micro: 0.457 f1_macro: 0.431\n",
      "Time taken: 52.48s\n",
      "Epoch 38\n",
      "Training loss (for one batch) at step 0: 0.5616\n",
      "Training loss (for one batch) at step 50: 0.7373\n",
      "Training loss (for one batch) at step 100: 0.4506\n",
      "Validation acc: 0.540 precision: 0.588 recall: 0.475 f1_micro: 0.525 f1_macro: 0.416\n",
      "Time taken: 52.44s\n",
      "Epoch 39\n",
      "Training loss (for one batch) at step 0: 0.4253\n",
      "Training loss (for one batch) at step 50: 0.3876\n",
      "Training loss (for one batch) at step 100: 0.3681\n",
      "Validation acc: 0.501 precision: 0.547 recall: 0.411 f1_micro: 0.469 f1_macro: 0.434\n",
      "Time taken: 52.46s\n",
      "Restoring best weights relative to validation accuracy...\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=40\n",
    "max_val_acc = 0.0\n",
    "best_weights = None\n",
    "\n",
    "l_iter = iter(labeled_ds)\n",
    "u_iter = iter(unlabeled_ds)\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f\"Epoch {epoch}\")\n",
    "  start_time = time.time()\n",
    "  for step in range(steps_per_epoch):\n",
    "    x, y = next(l_iter)\n",
    "    xu = next(u_iter)\n",
    "    \n",
    "    loss = train_step(x,y,xu)\n",
    "    if step % 50 == 0:\n",
    "      print(\n",
    "        \"Training loss (for one batch) at step %d: %.4f\"\n",
    "        % (step, float(loss))\n",
    "      )\n",
    "  for x_batch_val, y_batch_val in val_ds:\n",
    "      test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "  acc = float(val_acc_metric.result())\n",
    "  prec = float(val_prec_metric.result())\n",
    "  recall = float(val_recall_metric.result())\n",
    "  micro = float(f1_metric_micro.result())\n",
    "  macro = float(f1_metric_macro.result())\n",
    "\n",
    "  val_acc_metric.reset_states()\n",
    "  val_prec_metric.reset_states()\n",
    "  val_recall_metric.reset_states()\n",
    "  f1_metric_micro.reset_states()\n",
    "  f1_metric_macro.reset_states()\n",
    "\n",
    "  if acc > max_val_acc:\n",
    "    max_val_acc = acc\n",
    "    best_weights = model.get_weights()\n",
    "  print(f\"Validation acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")\n",
    "  print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "print(\"Restoring best weights relative to validation accuracy...\")\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.537 precision: 0.580 recall: 0.502 f1_micro: 0.538 f1_macro: 0.418\n"
     ]
    }
   ],
   "source": [
    "for x_batch_val, y_batch_val in test_ds:\n",
    "    test_step(x_batch_val, y_batch_val)\n",
    "acc = float(val_acc_metric.result())\n",
    "prec = float(val_prec_metric.result())\n",
    "recall = float(val_recall_metric.result())\n",
    "micro = float(f1_metric_micro.result())\n",
    "macro = float(f1_metric_macro.result())\n",
    "\n",
    "val_acc_metric.reset_states()\n",
    "val_prec_metric.reset_states()\n",
    "val_recall_metric.reset_states()\n",
    "f1_metric_micro.reset_states()\n",
    "f1_metric_macro.reset_states()\n",
    "print(f\"Test acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

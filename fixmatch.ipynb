{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IS_COLAB = True\n",
    "except:\n",
    "  IS_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "  !pip install tensorflow_addons\n",
    "  !pip install unidecode\n",
    "  !pip install transformers\n",
    "  !pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/SemiSupervised/*.csv ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 08:46:17.251496: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-29 08:46:17.775502: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64::/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:\n",
      "2023-04-29 08:46:17.775549: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64::/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:\n",
      "2023-04-29 08:46:17.775554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-29 08:46:18.411242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 08:46:18.411563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 08:46:18.415241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 08:46:18.415539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 08:46:18.415823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 08:46:18.416105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from unidecode import unidecode\n",
    "from utils import prepare_ds, prepare_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 08:47:04.372767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-29 08:47:04.373564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 08:47:04.373921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 08:47:04.374221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 08:47:04.881795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 08:47:04.882121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 08:47:04.882396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 08:47:04.882644: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-04-29 08:47:04.882668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6636 MB memory:  -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "labeled_ds, unlabeled_ds = prepare_train_ds(\"train_ner.csv\")\n",
    "test_ds = prepare_ds(\"test_ner.csv\")\n",
    "val_ds = prepare_ds(\"validation_internal_ner.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixMatchTune(tf.keras.Model):\n",
    "  def __init__(\n",
    "      self,\n",
    "      encoder_name=\"readerbench/RoBERT-base\",\n",
    "      num_classes=4,\n",
    "      **kwargs\n",
    "  ):\n",
    "    super(FixMatchTune,self).__init__(**kwargs)\n",
    "\n",
    "    self.bert = TFAutoModel.from_pretrained(encoder_name)\n",
    "    self.num_classes = num_classes\n",
    "    self.weak_augment = tf.keras.layers.GaussianNoise(stddev=0.5)\n",
    "    self.strong_augment = tf.keras.layers.GaussianNoise(stddev=5)\n",
    "\n",
    "    self.cls_head = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(256,activation=\"relu\"),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(self.num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    ids, mask = inputs\n",
    "    \n",
    "    embeds = self.bert(input_ids=ids, attention_mask=mask,training=training).pooler_output\n",
    "\n",
    "    strongs = self.strong_augment(embeds,training=training)\n",
    "    weaks = self.weak_augment(embeds,training=training)\n",
    "\n",
    "    strong_preds = self.cls_head(strongs,training=training)\n",
    "    weak_preds = self.cls_head(weaks,training=training)\n",
    "\n",
    "    return weak_preds, strong_preds\n",
    "  \n",
    "class FixMatch(tf.keras.Model):\n",
    "  def __init__(\n",
    "      self,\n",
    "      encoder_name=\"readerbench/RoBERT-base\",\n",
    "      num_classes=4,\n",
    "      encoder_train=False,\n",
    "      **kwargs\n",
    "  ):\n",
    "    super(FixMatch,self).__init__(**kwargs)\n",
    "\n",
    "    self.bert = TFAutoModel.from_pretrained(encoder_name)\n",
    "    self.bert.trainable = encoder_train\n",
    "    self.num_classes = num_classes\n",
    "    self.weak_augment = tf.keras.layers.GaussianNoise(stddev=0.5)\n",
    "    self.strong_augment = tf.keras.layers.GaussianNoise(stddev=5)\n",
    "\n",
    "\n",
    "    self.cls_head = tf.keras.Sequential([\n",
    "        tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(10),\n",
    "            merge_mode=\"ave\"\n",
    "        ),\n",
    "        tf.keras.layers.Dense(units=self.num_classes,activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    ids, mask = inputs\n",
    "    \n",
    "    embeds = self.bert(input_ids=ids, attention_mask=mask).last_hidden_state\n",
    "\n",
    "    strongs = self.strong_augment(embeds,training=training)\n",
    "    weaks = self.weak_augment(embeds,training=training)\n",
    "\n",
    "    strong_preds = self.cls_head(strongs)\n",
    "    weak_preds = self.cls_head(weaks)\n",
    "\n",
    "    return weak_preds, strong_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FixMatch()\n",
    "model = FixMatchTune()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tfa.optimizers.AdamW(weight_decay=0.001,learning_rate=0.005)\n",
    "optim2 = tfa.optimizers.AdamW(weight_decay=0.0,learning_rate=0.00001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_prec_metric = tf.keras.metrics.Precision(name=\"precision\")\n",
    "val_recall_metric = tf.keras.metrics.Recall(name=\"recall\")\n",
    "f1_metric_micro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='micro', name='f1_micro')\n",
    "f1_metric_macro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='macro', name='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.9\n",
    "unsup_weight = 1.0\n",
    "steps_per_epoch = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_tune(x,y,xu):\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      wl,_ = model([x[\"input_ids\"],x[\"attention_mask\"]],training=True)\n",
    "      wu,su = model([xu[\"input_ids\"],xu[\"attention_mask\"]],training=True)\n",
    "      ls = loss_fn(y, wl)\n",
    "      mask = tf.reduce_max(wu,axis=1)>thresh\n",
    "      wu = wu[mask]\n",
    "      su = su[mask]\n",
    "      lu = loss_fn(tf.argmax(wu,axis=1),su)\n",
    "\n",
    "      loss = ls + unsup_weight * lu\n",
    "\n",
    "  grads = tape.gradient(loss, [model.cls_head.trainable_weights, model.bert.trainable_weights])\n",
    "  optim.apply_gradients(zip(grads[0], model.cls_head.trainable_weights))\n",
    "  optim2.apply_gradients(zip(grads[1], model.bert.trainable_weights))\n",
    "\n",
    "  return loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(x,y,xu):\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      wl,_ = model([x[\"input_ids\"],x[\"attention_mask\"]],training=True)\n",
    "      wu,su = model([xu[\"input_ids\"],xu[\"attention_mask\"]],training=True)\n",
    "      ls = loss_fn(y, wl)\n",
    "      mask = tf.reduce_max(wu,axis=1)>thresh\n",
    "      wu = wu[mask]\n",
    "      su = su[mask]\n",
    "      lu = loss_fn(tf.argmax(wu,axis=1),su)\n",
    "\n",
    "      loss = ls + unsup_weight * lu\n",
    "\n",
    "  grads = tape.gradient(loss, model.trainable_weights)\n",
    "  optim.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "  return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    wpred,_ = model([x[\"input_ids\"],x[\"attention_mask\"]], training=False)\n",
    "    val_acc_metric.update_state(y, wpred)\n",
    "    true_hot = tf.one_hot(y, 4)\n",
    "    val_prec_metric.update_state(true_hot, wpred)\n",
    "    val_recall_metric.update_state(true_hot, wpred)\n",
    "    f1_metric_micro.update_state(true_hot, wpred)\n",
    "    f1_metric_macro.update_state(true_hot, wpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training loss (for one batch) at step 0: 1.7838\n",
      "Training loss (for one batch) at step 50: 1.2648\n",
      "Training loss (for one batch) at step 100: 1.1871\n",
      "Training loss (for one batch) at step 150: 1.0920\n",
      "Training loss (for one batch) at step 200: 0.5658\n",
      "Training loss (for one batch) at step 250: 5.4454\n",
      "Training loss (for one batch) at step 300: 0.7656\n",
      "Training loss (for one batch) at step 350: 0.7228\n",
      "Training loss (for one batch) at step 400: 6.4009\n",
      "Training loss (for one batch) at step 450: 0.7365\n",
      "Training loss (for one batch) at step 500: 1.3853\n",
      "Training loss (for one batch) at step 550: 0.6015\n",
      "Validation acc: 0.583 precision: 0.604 recall: 0.572 f1_micro: 0.588 f1_macro: 0.524\n",
      "Time taken: 228.88s\n",
      "Epoch 1\n",
      "Training loss (for one batch) at step 0: 0.2515\n",
      "Training loss (for one batch) at step 50: 0.5211\n",
      "Training loss (for one batch) at step 100: 0.2893\n",
      "Training loss (for one batch) at step 150: 0.4689\n",
      "Training loss (for one batch) at step 200: 0.5770\n",
      "Training loss (for one batch) at step 250: 0.4321\n",
      "Training loss (for one batch) at step 300: 0.4545\n",
      "Training loss (for one batch) at step 350: 0.2847\n",
      "Training loss (for one batch) at step 400: 0.4568\n",
      "Training loss (for one batch) at step 450: 0.3632\n",
      "Training loss (for one batch) at step 500: 0.5724\n",
      "Training loss (for one batch) at step 550: 0.2364\n",
      "Validation acc: 0.646 precision: 0.653 recall: 0.642 f1_micro: 0.648 f1_macro: 0.617\n",
      "Time taken: 208.72s\n",
      "Epoch 2\n",
      "Training loss (for one batch) at step 0: 0.5055\n",
      "Training loss (for one batch) at step 50: 0.7871\n",
      "Training loss (for one batch) at step 100: 0.2010\n",
      "Training loss (for one batch) at step 150: 0.1234\n",
      "Training loss (for one batch) at step 200: 0.5760\n",
      "Training loss (for one batch) at step 250: 0.4225\n",
      "Training loss (for one batch) at step 300: 0.0910\n",
      "Training loss (for one batch) at step 350: 0.0531\n",
      "Training loss (for one batch) at step 400: 0.5023\n",
      "Training loss (for one batch) at step 450: 0.0841\n",
      "Training loss (for one batch) at step 500: 0.3547\n",
      "Training loss (for one batch) at step 550: 0.0320\n",
      "Validation acc: 0.663 precision: 0.663 recall: 0.663 f1_micro: 0.663 f1_macro: 0.650\n",
      "Time taken: 209.05s\n",
      "Epoch 3\n",
      "Training loss (for one batch) at step 0: 0.0447\n",
      "Training loss (for one batch) at step 50: 0.1881\n",
      "Training loss (for one batch) at step 100: 0.0992\n",
      "Training loss (for one batch) at step 150: 0.0598\n",
      "Training loss (for one batch) at step 200: 0.2274\n",
      "Training loss (for one batch) at step 250: 0.0562\n",
      "Training loss (for one batch) at step 300: 0.6084\n",
      "Training loss (for one batch) at step 350: 0.1371\n",
      "Training loss (for one batch) at step 400: 0.0434\n",
      "Training loss (for one batch) at step 450: 0.8234\n",
      "Training loss (for one batch) at step 500: 0.7493\n",
      "Training loss (for one batch) at step 550: 0.0265\n",
      "Validation acc: 0.651 precision: 0.652 recall: 0.650 f1_micro: 0.651 f1_macro: 0.648\n",
      "Time taken: 208.96s\n",
      "Epoch 4\n",
      "Training loss (for one batch) at step 0: 0.2502\n",
      "Training loss (for one batch) at step 50: 0.0660\n",
      "Training loss (for one batch) at step 100: 0.3617\n",
      "Training loss (for one batch) at step 150: 0.0416\n",
      "Training loss (for one batch) at step 200: 0.4104\n",
      "Training loss (for one batch) at step 250: 0.2736\n",
      "Training loss (for one batch) at step 300: 0.0305\n",
      "Training loss (for one batch) at step 350: 0.5440\n",
      "Training loss (for one batch) at step 400: 0.0531\n",
      "Training loss (for one batch) at step 450: 0.2439\n",
      "Training loss (for one batch) at step 500: 0.0054\n",
      "Training loss (for one batch) at step 550: 0.1474\n",
      "Validation acc: 0.643 precision: 0.644 recall: 0.642 f1_micro: 0.643 f1_macro: 0.631\n",
      "Time taken: 208.98s\n",
      "Epoch 5\n",
      "Training loss (for one batch) at step 0: 0.0383\n",
      "Training loss (for one batch) at step 50: 0.0794\n",
      "Training loss (for one batch) at step 100: 0.0471\n",
      "Training loss (for one batch) at step 150: 0.3025\n",
      "Training loss (for one batch) at step 200: 0.4927\n",
      "Training loss (for one batch) at step 250: 0.3372\n",
      "Training loss (for one batch) at step 300: 0.0376\n",
      "Training loss (for one batch) at step 350: 0.2380\n",
      "Training loss (for one batch) at step 400: 0.0191\n",
      "Training loss (for one batch) at step 450: 0.6002\n",
      "Training loss (for one batch) at step 500: 0.5495\n",
      "Training loss (for one batch) at step 550: 0.0083\n",
      "Validation acc: 0.675 precision: 0.675 recall: 0.675 f1_micro: 0.675 f1_macro: 0.676\n",
      "Time taken: 209.14s\n",
      "Epoch 6\n",
      "Training loss (for one batch) at step 0: 0.0480\n",
      "Training loss (for one batch) at step 50: 0.0451\n",
      "Training loss (for one batch) at step 100: 0.0140\n",
      "Training loss (for one batch) at step 150: 0.0193\n",
      "Training loss (for one batch) at step 200: 0.1845\n",
      "Training loss (for one batch) at step 250: 0.0129\n",
      "Training loss (for one batch) at step 300: 0.3604\n",
      "Training loss (for one batch) at step 350: 0.0724\n",
      "Training loss (for one batch) at step 400: 0.6767\n",
      "Training loss (for one batch) at step 450: 1.0529\n",
      "Training loss (for one batch) at step 500: 0.3949\n",
      "Training loss (for one batch) at step 550: 0.8085\n",
      "Validation acc: 0.602 precision: 0.608 recall: 0.601 f1_micro: 0.604 f1_macro: 0.611\n",
      "Time taken: 208.98s\n",
      "Epoch 7\n",
      "Training loss (for one batch) at step 0: 0.4437\n",
      "Training loss (for one batch) at step 50: 0.1096\n",
      "Training loss (for one batch) at step 100: 0.0269\n",
      "Training loss (for one batch) at step 150: 0.0294\n",
      "Training loss (for one batch) at step 200: 0.4278\n",
      "Training loss (for one batch) at step 250: 0.0245\n",
      "Training loss (for one batch) at step 300: 0.0306\n",
      "Training loss (for one batch) at step 350: 0.3551\n",
      "Training loss (for one batch) at step 400: 0.3700\n",
      "Training loss (for one batch) at step 450: 0.0053\n",
      "Training loss (for one batch) at step 500: 0.0364\n",
      "Training loss (for one batch) at step 550: 0.0234\n",
      "Validation acc: 0.689 precision: 0.689 recall: 0.689 f1_micro: 0.689 f1_macro: 0.680\n",
      "Time taken: 209.13s\n",
      "Epoch 8\n",
      "Training loss (for one batch) at step 0: 0.0597\n",
      "Training loss (for one batch) at step 50: 0.0294\n",
      "Training loss (for one batch) at step 100: 0.0070\n",
      "Training loss (for one batch) at step 150: 0.1139\n",
      "Training loss (for one batch) at step 200: 0.0309\n",
      "Training loss (for one batch) at step 250: 0.1085\n",
      "Training loss (for one batch) at step 300: 0.3959\n",
      "Training loss (for one batch) at step 350: 0.0736\n",
      "Training loss (for one batch) at step 400: 0.2357\n",
      "Training loss (for one batch) at step 450: 0.0130\n",
      "Training loss (for one batch) at step 500: 0.1020\n",
      "Training loss (for one batch) at step 550: 0.0121\n",
      "Validation acc: 0.618 precision: 0.618 recall: 0.618 f1_micro: 0.618 f1_macro: 0.613\n",
      "Time taken: 209.01s\n",
      "Epoch 9\n",
      "Training loss (for one batch) at step 0: 0.0088\n",
      "Training loss (for one batch) at step 50: 0.0241\n",
      "Training loss (for one batch) at step 100: 0.0408\n",
      "Training loss (for one batch) at step 150: 0.0087\n",
      "Training loss (for one batch) at step 200: 0.0989\n",
      "Training loss (for one batch) at step 250: 0.0632\n",
      "Training loss (for one batch) at step 300: 0.5913\n",
      "Training loss (for one batch) at step 350: 0.0242\n",
      "Training loss (for one batch) at step 400: 0.0205\n",
      "Training loss (for one batch) at step 450: 0.0308\n",
      "Training loss (for one batch) at step 500: 0.2368\n",
      "Training loss (for one batch) at step 550: 0.0380\n",
      "Validation acc: 0.653 precision: 0.654 recall: 0.653 f1_micro: 0.653 f1_macro: 0.608\n",
      "Time taken: 209.00s\n",
      "Epoch 10\n",
      "Training loss (for one batch) at step 0: 0.0088\n",
      "Training loss (for one batch) at step 50: 0.4594\n",
      "Training loss (for one batch) at step 100: 0.2729\n",
      "Training loss (for one batch) at step 150: 0.0839\n",
      "Training loss (for one batch) at step 200: 0.3404\n",
      "Training loss (for one batch) at step 250: 0.1040\n",
      "Training loss (for one batch) at step 300: 0.0006\n",
      "Training loss (for one batch) at step 350: 0.1004\n",
      "Training loss (for one batch) at step 400: 0.0102\n",
      "Training loss (for one batch) at step 450: 0.0190\n",
      "Training loss (for one batch) at step 500: 0.0027\n",
      "Training loss (for one batch) at step 550: 0.0006\n",
      "Validation acc: 0.666 precision: 0.667 recall: 0.665 f1_micro: 0.666 f1_macro: 0.650\n",
      "Time taken: 209.00s\n",
      "Epoch 11\n",
      "Training loss (for one batch) at step 0: 0.0879\n",
      "Training loss (for one batch) at step 50: 0.0026\n",
      "Training loss (for one batch) at step 100: 0.0473\n",
      "Training loss (for one batch) at step 150: 0.0041\n",
      "Training loss (for one batch) at step 200: 0.3271\n",
      "Training loss (for one batch) at step 250: 0.1810\n",
      "Training loss (for one batch) at step 300: 0.0205\n",
      "Training loss (for one batch) at step 350: 0.1344\n",
      "Training loss (for one batch) at step 400: 0.5621\n",
      "Training loss (for one batch) at step 450: 1.4539\n",
      "Training loss (for one batch) at step 500: 0.7721\n",
      "Training loss (for one batch) at step 550: 0.9357\n",
      "Validation acc: 0.539 precision: 0.771 recall: 0.315 f1_micro: 0.448 f1_macro: 0.239\n",
      "Time taken: 208.93s\n",
      "Epoch 12\n",
      "Training loss (for one batch) at step 0: 0.8105\n",
      "Training loss (for one batch) at step 50: 0.3640\n",
      "Training loss (for one batch) at step 100: 0.5203\n",
      "Training loss (for one batch) at step 150: 0.7701\n",
      "Training loss (for one batch) at step 200: 0.6416\n",
      "Training loss (for one batch) at step 250: 0.1907\n",
      "Training loss (for one batch) at step 300: 0.1332\n",
      "Training loss (for one batch) at step 350: 0.3087\n",
      "Training loss (for one batch) at step 400: 0.0994\n",
      "Training loss (for one batch) at step 450: 0.3898\n",
      "Training loss (for one batch) at step 500: 0.9298\n",
      "Training loss (for one batch) at step 550: 0.5867\n",
      "Validation acc: 0.547 precision: 0.547 recall: 0.546 f1_micro: 0.547 f1_macro: 0.555\n",
      "Time taken: 209.27s\n",
      "Epoch 13\n",
      "Training loss (for one batch) at step 0: 0.2913\n",
      "Training loss (for one batch) at step 50: 0.4630\n",
      "Training loss (for one batch) at step 100: 0.1012\n",
      "Training loss (for one batch) at step 150: 0.0680\n",
      "Training loss (for one batch) at step 200: 0.0527\n",
      "Training loss (for one batch) at step 250: 0.0478\n",
      "Training loss (for one batch) at step 300: 0.0281\n",
      "Training loss (for one batch) at step 350: 0.8874\n",
      "Training loss (for one batch) at step 400: 0.0195\n",
      "Training loss (for one batch) at step 450: 0.0499\n",
      "Training loss (for one batch) at step 500: 0.0614\n",
      "Training loss (for one batch) at step 550: 0.2518\n",
      "Validation acc: 0.665 precision: 0.666 recall: 0.665 f1_micro: 0.665 f1_macro: 0.662\n",
      "Time taken: 209.72s\n",
      "Epoch 14\n",
      "Training loss (for one batch) at step 0: 0.0180\n",
      "Training loss (for one batch) at step 50: 0.3163\n",
      "Training loss (for one batch) at step 100: 0.0381\n",
      "Training loss (for one batch) at step 150: 0.0539\n",
      "Training loss (for one batch) at step 200: 0.4278\n",
      "Training loss (for one batch) at step 250: 0.0359\n",
      "Training loss (for one batch) at step 300: 0.0371\n",
      "Training loss (for one batch) at step 350: 0.0027\n",
      "Training loss (for one batch) at step 400: 0.1291\n",
      "Training loss (for one batch) at step 450: 0.0762\n",
      "Training loss (for one batch) at step 500: 0.0200\n",
      "Training loss (for one batch) at step 550: 0.0650\n",
      "Validation acc: 0.688 precision: 0.688 recall: 0.688 f1_micro: 0.688 f1_macro: 0.659\n",
      "Time taken: 209.67s\n",
      "Epoch 15\n",
      "Training loss (for one batch) at step 0: 0.2290\n",
      "Training loss (for one batch) at step 50: 0.0096\n",
      "Training loss (for one batch) at step 100: 0.1614\n",
      "Training loss (for one batch) at step 150: 0.0145\n",
      "Training loss (for one batch) at step 200: 0.0039\n",
      "Training loss (for one batch) at step 250: 0.1001\n",
      "Training loss (for one batch) at step 300: 0.0008\n",
      "Training loss (for one batch) at step 350: 0.2082\n",
      "Training loss (for one batch) at step 400: 0.0148\n",
      "Training loss (for one batch) at step 450: 0.0496\n",
      "Training loss (for one batch) at step 500: 0.0171\n",
      "Training loss (for one batch) at step 550: 0.0555\n",
      "Validation acc: 0.636 precision: 0.636 recall: 0.636 f1_micro: 0.636 f1_macro: 0.630\n",
      "Time taken: 209.71s\n",
      "Restoring best weights relative to validation accuracy...\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=16\n",
    "max_val_acc = 0.0\n",
    "best_weights = None\n",
    "\n",
    "l_iter = iter(labeled_ds)\n",
    "u_iter = iter(unlabeled_ds)\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f\"Epoch {epoch}\")\n",
    "  start_time = time.time()\n",
    "  for step in range(steps_per_epoch):\n",
    "    x, y = next(l_iter)\n",
    "    xu = next(u_iter)\n",
    "    \n",
    "    loss = train_step_tune(x,y,xu)\n",
    "    if step % 50 == 0:\n",
    "      print(\n",
    "        \"Training loss (for one batch) at step %d: %.4f\"\n",
    "        % (step, float(loss))\n",
    "      )\n",
    "  for x_batch_val, y_batch_val in val_ds:\n",
    "      test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "  acc = float(val_acc_metric.result())\n",
    "  prec = float(val_prec_metric.result())\n",
    "  recall = float(val_recall_metric.result())\n",
    "  micro = float(f1_metric_micro.result())\n",
    "  macro = float(f1_metric_macro.result())\n",
    "\n",
    "  val_acc_metric.reset_states()\n",
    "  val_prec_metric.reset_states()\n",
    "  val_recall_metric.reset_states()\n",
    "  f1_metric_micro.reset_states()\n",
    "  f1_metric_macro.reset_states()\n",
    "\n",
    "  if acc > max_val_acc:\n",
    "    max_val_acc = acc\n",
    "    best_weights = model.get_weights()\n",
    "  print(f\"Validation acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")\n",
    "  print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "print(\"Restoring best weights relative to validation accuracy...\")\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.687 precision: 0.687 recall: 0.687 f1_micro: 0.687 f1_macro: 0.680\n"
     ]
    }
   ],
   "source": [
    "for x_batch_val, y_batch_val in test_ds:\n",
    "    test_step(x_batch_val, y_batch_val)\n",
    "acc = float(val_acc_metric.result())\n",
    "prec = float(val_prec_metric.result())\n",
    "recall = float(val_recall_metric.result())\n",
    "micro = float(f1_metric_micro.result())\n",
    "macro = float(f1_metric_macro.result())\n",
    "\n",
    "val_acc_metric.reset_states()\n",
    "val_prec_metric.reset_states()\n",
    "val_recall_metric.reset_states()\n",
    "f1_metric_micro.reset_states()\n",
    "f1_metric_macro.reset_states()\n",
    "print(f\"Test acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./checkpoints/fixmatch_tune\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

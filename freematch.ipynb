{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IS_COLAB = True\n",
    "except:\n",
    "  IS_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "  !pip install tensorflow_addons\n",
    "  !pip install unidecode\n",
    "  !pip install transformers\n",
    "  !pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/SemiSupervised/*.csv ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train_ner.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    s = unidecode(x)\n",
    "    s = str.lower(s)\n",
    "    s = re.sub(r\"\\[[a-z]+\\]\",\"\", s)\n",
    "    s = re.sub(r\"\\*\",\"\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9]+\",\" \",s)\n",
    "    s = re.sub(r\" +\",\" \",s)\n",
    "    s = re.sub(r\"(.)\\1+\",r\"\\1\",s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABUSE': 0, 'INSULT': 1, 'OTHER': 2, 'PROFANITY': 3}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_ids = {label_name:i for i, label_name in enumerate(sorted(set(data[\"label\"])))}\n",
    "label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32674</td>\n",
       "      <td>da de unde stii u mai [ORG] ca banii au fost p...</td>\n",
       "      <td>ABUSE</td>\n",
       "      <td>da de unde sti u mai ca bani au fost pt si nu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16514</td>\n",
       "      <td>m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>muie muie muie bai kakatule stai in banca ta d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32556</td>\n",
       "      <td>PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>pt ala care are treaba cu esti unul care nu ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23861</td>\n",
       "      <td>sunt bucuros ca [PERS] nu a mai venit la [ORG]...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>sunt bucuros ca nu a mai venit la jucator de d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21811</td>\n",
       "      <td>[PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>esti pe n es cu li bi l te asemeni cu de la c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text   label  \\\n",
       "0  32674  da de unde stii u mai [ORG] ca banii au fost p...   ABUSE   \n",
       "1  16514  m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...  INSULT   \n",
       "2  32556  PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...   OTHER   \n",
       "3  23861  sunt bucuros ca [PERS] nu a mai venit la [ORG]...   OTHER   \n",
       "4  21811  [PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...  INSULT   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  da de unde sti u mai ca bani au fost pt si nu ...  \n",
       "1  muie muie muie bai kakatule stai in banca ta d...  \n",
       "2  pt ala care are treaba cu esti unul care nu ar...  \n",
       "3  sunt bucuros ca nu a mai venit la jucator de d...  \n",
       "4   esti pe n es cu li bi l te asemeni cu de la c...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"preprocessed\"] = data[\"text\"].apply(preprocess)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32674</td>\n",
       "      <td>da de unde stii u mai [ORG] ca banii au fost p...</td>\n",
       "      <td>ABUSE</td>\n",
       "      <td>da de unde sti u mai ca bani au fost pt si nu ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16514</td>\n",
       "      <td>m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>muie muie muie bai kakatule stai in banca ta d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32556</td>\n",
       "      <td>PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>pt ala care are treaba cu esti unul care nu ar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23861</td>\n",
       "      <td>sunt bucuros ca [PERS] nu a mai venit la [ORG]...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>sunt bucuros ca nu a mai venit la jucator de d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21811</td>\n",
       "      <td>[PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...</td>\n",
       "      <td>INSULT</td>\n",
       "      <td>esti pe n es cu li bi l te asemeni cu de la c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text   label  \\\n",
       "0  32674  da de unde stii u mai [ORG] ca banii au fost p...   ABUSE   \n",
       "1  16514  m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...  INSULT   \n",
       "2  32556  PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...   OTHER   \n",
       "3  23861  sunt bucuros ca [PERS] nu a mai venit la [ORG]...   OTHER   \n",
       "4  21811  [PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...  INSULT   \n",
       "\n",
       "                                        preprocessed  class  \n",
       "0  da de unde sti u mai ca bani au fost pt si nu ...      0  \n",
       "1  muie muie muie bai kakatule stai in banca ta d...      1  \n",
       "2  pt ala care are treaba cu esti unul care nu ar...      2  \n",
       "3  sunt bucuros ca nu a mai venit la jucator de d...      2  \n",
       "4   esti pe n es cu li bi l te asemeni cu de la c...      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"class\"] = data[\"label\"].map(lambda x: label_ids[x])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OTHER        3649\n",
       "ABUSE        2768\n",
       "INSULT       2242\n",
       "PROFANITY    1294\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_robert = AutoTokenizer.from_pretrained(\"readerbench/RoBERT-base\")\n",
    "robert = TFAutoModel.from_pretrained(\"readerbench/RoBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ssl_data(ids_array,mask_array,labels,num_classes,label_percent):\n",
    "  labeled = None\n",
    "  unlabeled = None\n",
    "\n",
    "  for class_idx in range(num_classes):\n",
    "    class_ids = ids_array[labels==class_idx]\n",
    "    class_mask = mask_array[labels==class_idx]\n",
    "    sz = int(label_percent * class_ids.shape[0])\n",
    "\n",
    "    labels_reduced = labels[labels==class_idx][:sz]\n",
    "    labeled_ids, unlabeled_ids = class_ids[:sz], class_ids[sz:]\n",
    "    labeled_mask, unlabeled_mask = class_mask[:sz], class_mask[sz:]\n",
    "\n",
    "    if not labeled:\n",
    "      labeled = (labeled_ids, labeled_mask, labels_reduced)\n",
    "      unlabeled = (unlabeled_ids, unlabeled_mask)\n",
    "    else:\n",
    "      labeled = (\n",
    "          np.concatenate([labeled[0],labeled_ids]),\n",
    "          np.concatenate([labeled[1],labeled_mask]),\n",
    "          np.concatenate([labeled[2],labels_reduced])\n",
    "      )\n",
    "      unlabeled = (\n",
    "          np.concatenate([unlabeled[0],unlabeled_ids]),\n",
    "          np.concatenate([unlabeled[1],unlabeled_mask]),\n",
    "      )\n",
    "\n",
    "  return labeled, unlabeled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_robert(x):\n",
    "  t = tok_robert(x,padding=\"max_length\",max_length=96,truncation=True,return_tensors='np')\n",
    "  return t[\"input_ids\"], t[\"attention_mask\"]\n",
    "\n",
    "def map_func(input_ids, masks, labels):\n",
    "  return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "\n",
    "def map_func2(input_ids, masks):\n",
    "  return {'input_ids': input_ids, 'attention_mask': masks}\n",
    "\n",
    "def prepare_ds(filename,batch_size=64,label_percent=0.8):\n",
    "  df = pd.read_csv(filename)\n",
    "  X_id_mask = df['text'].map(preprocess).apply(preprocess_robert).apply(pd.Series)\n",
    "\n",
    "  X_id_mask.columns = [\"input_ids\",\"attention_mask\"]\n",
    "\n",
    "  ids_array = np.squeeze(np.stack(X_id_mask.input_ids.values), axis=1)\n",
    "  mask_array = np.squeeze(np.stack(X_id_mask.attention_mask.values), axis=1)\n",
    "  labels = df[\"label\"].map(lambda x: label_ids[x]).values\n",
    "\n",
    "  res_ds = tf.data.Dataset.from_tensor_slices((ids_array, mask_array, labels)).map(map_func).shuffle(len(df)).batch(batch_size)\n",
    "  return res_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_ds(filename,batch_size=16,label_percent=0.05):\n",
    "  df = pd.read_csv(filename)\n",
    "  X_id_mask = df['text'].map(preprocess).apply(preprocess_robert).apply(pd.Series)\n",
    "\n",
    "  X_id_mask.columns = [\"input_ids\",\"attention_mask\"]\n",
    "\n",
    "  ids_array = np.squeeze(np.stack(X_id_mask.input_ids.values), axis=1)\n",
    "  mask_array = np.squeeze(np.stack(X_id_mask.attention_mask.values), axis=1)\n",
    "  labels = df[\"label\"].map(lambda x: label_ids[x]).values\n",
    "\n",
    "  labeled, unlabeled = split_ssl_data(ids_array,mask_array,labels,len(label_ids),label_percent)\n",
    "  labeled_ds = tf.data.Dataset.from_tensor_slices(labeled)\n",
    "  labeled_ds = labeled_ds.map(map_func).shuffle(len(labeled_ds)).batch(batch_size).repeat()\n",
    "  unlabeled_ds = tf.data.Dataset.from_tensor_slices(unlabeled)\n",
    "  unlabeled_ds = unlabeled_ds.map(map_func2).shuffle(len(unlabeled_ds)).batch(4*batch_size).repeat()\n",
    "  \n",
    "  return labeled_ds, unlabeled_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ds, unlabeled_ds = prepare_train_ds(\"train_ner.csv\")\n",
    "test_ds = prepare_ds(\"test_ner.csv\")\n",
    "val_ds = prepare_ds(\"validation_internal_ner.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreeMatch(tf.keras.Model):\n",
    "  def __init__(self,bert_model,num_classes=4,**kwargs):\n",
    "    super(FreeMatch,self).__init__(**kwargs)\n",
    "    self.bert = bert_model\n",
    "    self.bert.trainable = False\n",
    "    self.num_classes = num_classes\n",
    "    self.weak_augment = tf.keras.layers.GaussianNoise(stddev=0.5)\n",
    "    self.strong_augment = tf.keras.layers.GaussianNoise(stddev=5)\n",
    "\n",
    "\n",
    "    self.cls_head = tf.keras.Sequential([\n",
    "        tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(10),\n",
    "            merge_mode=\"ave\"\n",
    "        ),\n",
    "        tf.keras.layers.Dense(units=self.num_classes,activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    ids, mask = inputs\n",
    "    \n",
    "    embeds = self.bert(input_ids=ids, attention_mask=mask).last_hidden_state\n",
    "\n",
    "    strongs = self.strong_augment(embeds,training=training)\n",
    "    weaks = self.weak_augment(embeds,training=training)\n",
    "\n",
    "    strong_preds = self.cls_head(strongs)\n",
    "    weak_preds = self.cls_head(weaks)\n",
    "\n",
    "    return weak_preds, strong_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FreeMatch(bert_model=robert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tfa.optimizers.AdamW(weight_decay=0.001,learning_rate=0.005)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_prec_metric = tf.keras.metrics.Precision(name=\"precision\")\n",
    "val_recall_metric = tf.keras.metrics.Recall(name=\"recall\")\n",
    "f1_metric_micro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='micro', name='f1_micro')\n",
    "f1_metric_macro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='macro', name='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.9\n",
    "unsup_weight = 1.0\n",
    "num_classes = len(label_ids)\n",
    "fairness_weight = 0.01\n",
    "eps = 1e-4\n",
    "steps_per_epoch = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = tf.Variable(tf.constant(1/num_classes))\n",
    "p_thresh = tf.Variable(tau * tf.ones((num_classes,),dtype=tf.float32))\n",
    "hist_t = tf.Variable(eps * tf.ones((num_classes,),dtype=tf.float32))\n",
    "eps_t = tf.Variable(eps * tf.ones((num_classes,),dtype=tf.float32))\n",
    "\n",
    "ema_decay = tf.constant(0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sumnorm(x):\n",
    "  return x/tf.reduce_sum(x)\n",
    "\n",
    "@tf.function\n",
    "def maxnorm(x):\n",
    "  return x/tf.reduce_max(x)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x,y,xu,ema_decay):\n",
    "  with tf.GradientTape() as tape:\n",
    "      wl,_ = model([x[\"input_ids\"],x[\"attention_mask\"]],training=True)\n",
    "      wu,su = model([xu[\"input_ids\"],xu[\"attention_mask\"]],training=True)\n",
    "\n",
    "# 2 from paper\n",
    "      ls = loss_fn(y, wl)\n",
    "\n",
    "# 3 \n",
    "      local_thresh_update = tf.reduce_mean(tf.reduce_max(wu,axis=1))\n",
    "      tau.assign(ema_decay * tau + (1-ema_decay) * local_thresh_update)\n",
    "\n",
    "# 4\n",
    "      p_thresh.assign(ema_decay * p_thresh + (1-ema_decay) * tf.reduce_mean(wu,axis=0))\n",
    "\n",
    "# 5\n",
    "      hist_t.assign(ema_decay * hist_t + (1-ema_decay) * tf.reduce_sum(tf.one_hot(tf.argmax(wu,axis=1),4),axis=0))\n",
    "\n",
    "# 6\n",
    "      sat = tau * maxnorm(p_thresh)\n",
    "\n",
    "# 7\n",
    "      mask = tf.reduce_max(wu,axis=1)>=tf.gather(sat,tf.argmax(wu,axis=1))\n",
    "      \n",
    "      su = su[mask]\n",
    "      wu = wu[mask]\n",
    "      \n",
    "\n",
    "      lu = loss_fn(tf.argmax(wu,axis=1),su)\n",
    "      \n",
    "      p_avg = tf.reduce_mean(su,axis=0)\n",
    "      h_avg = tf.reduce_sum(tf.one_hot(tf.argmax(su,axis=1),4),axis=0)\n",
    "\n",
    "      lf = -tf.keras.metrics.categorical_crossentropy(sumnorm(p_thresh/hist_t),sumnorm(p_avg/(eps_t+h_avg)))\n",
    "\n",
    "      loss = ls + unsup_weight * lu + fairness_weight * lf\n",
    "\n",
    "  grads = tape.gradient(loss, model.trainable_weights)\n",
    "  optim.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "  return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    wpred,_ = model([x[\"input_ids\"],x[\"attention_mask\"]], training=False)\n",
    "    val_acc_metric.update_state(y, wpred)\n",
    "    true_hot = tf.one_hot(y, 4)\n",
    "    val_prec_metric.update_state(true_hot, wpred)\n",
    "    val_recall_metric.update_state(true_hot, wpred)\n",
    "    f1_metric_micro.update_state(true_hot, wpred)\n",
    "    f1_metric_macro.update_state(true_hot, wpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 08:34:42.535591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 2.6781\n",
      "Training loss (for one batch) at step 50: 2.0876\n",
      "Training loss (for one batch) at step 100: 2.3032\n",
      "Training loss (for one batch) at step 150: 2.2053\n",
      "Validation acc: 0.510 precision: 0.654 recall: 0.236 f1_micro: 0.346 f1_macro: 0.163\n",
      "Time taken: 76.35s\n",
      "Epoch 1\n",
      "Training loss (for one batch) at step 0: 1.7199\n",
      "Training loss (for one batch) at step 50: 1.8361\n",
      "Training loss (for one batch) at step 100: 2.3641\n",
      "Training loss (for one batch) at step 150: 2.2281\n",
      "Validation acc: 0.513 precision: 0.635 recall: 0.274 f1_micro: 0.383 f1_macro: 0.209\n",
      "Time taken: 52.86s\n",
      "Epoch 2\n",
      "Training loss (for one batch) at step 0: 2.3985\n",
      "Training loss (for one batch) at step 50: 2.3266\n",
      "Training loss (for one batch) at step 100: 2.1774\n",
      "Training loss (for one batch) at step 150: 2.0808\n",
      "Validation acc: 0.505 precision: 0.569 recall: 0.396 f1_micro: 0.467 f1_macro: 0.295\n",
      "Time taken: 52.78s\n",
      "Epoch 3\n",
      "Training loss (for one batch) at step 0: 2.1718\n",
      "Training loss (for one batch) at step 50: 1.2944\n",
      "Training loss (for one batch) at step 100: 1.6531\n",
      "Training loss (for one batch) at step 150: 2.1858\n",
      "Validation acc: 0.528 precision: 0.688 recall: 0.303 f1_micro: 0.421 f1_macro: 0.263\n",
      "Time taken: 53.43s\n",
      "Epoch 4\n",
      "Training loss (for one batch) at step 0: 1.9154\n",
      "Training loss (for one batch) at step 50: 1.7449\n",
      "Training loss (for one batch) at step 100: 1.8946\n",
      "Training loss (for one batch) at step 150: 1.7406\n",
      "Validation acc: 0.520 precision: 0.622 recall: 0.353 f1_micro: 0.450 f1_macro: 0.317\n",
      "Time taken: 53.85s\n",
      "Epoch 5\n",
      "Training loss (for one batch) at step 0: 1.8558\n",
      "Training loss (for one batch) at step 50: 2.0156\n",
      "Training loss (for one batch) at step 100: 2.4886\n",
      "Training loss (for one batch) at step 150: 1.9706\n",
      "Validation acc: 0.537 precision: 0.641 recall: 0.399 f1_micro: 0.492 f1_macro: 0.354\n",
      "Time taken: 54.05s\n",
      "Epoch 6\n",
      "Training loss (for one batch) at step 0: 1.8517\n",
      "Training loss (for one batch) at step 50: 1.9308\n",
      "Training loss (for one batch) at step 100: 1.6579\n",
      "Training loss (for one batch) at step 150: 1.6669\n",
      "Validation acc: 0.512 precision: 0.629 recall: 0.384 f1_micro: 0.477 f1_macro: 0.373\n",
      "Time taken: 53.92s\n",
      "Epoch 7\n",
      "Training loss (for one batch) at step 0: 1.7564\n",
      "Training loss (for one batch) at step 50: 1.7446\n",
      "Training loss (for one batch) at step 100: 1.8081\n",
      "Training loss (for one batch) at step 150: 1.8711\n",
      "Validation acc: 0.545 precision: 0.624 recall: 0.424 f1_micro: 0.505 f1_macro: 0.407\n",
      "Time taken: 54.01s\n",
      "Epoch 8\n",
      "Training loss (for one batch) at step 0: 1.8263\n",
      "Training loss (for one batch) at step 50: 1.5467\n",
      "Training loss (for one batch) at step 100: 1.7307\n",
      "Training loss (for one batch) at step 150: 1.7617\n",
      "Validation acc: 0.509 precision: 0.565 recall: 0.416 f1_micro: 0.480 f1_macro: 0.418\n",
      "Time taken: 53.99s\n",
      "Epoch 9\n",
      "Training loss (for one batch) at step 0: 2.2412\n",
      "Training loss (for one batch) at step 50: 1.9274\n",
      "Training loss (for one batch) at step 100: 2.3653\n",
      "Training loss (for one batch) at step 150: 1.5781\n",
      "Validation acc: 0.542 precision: 0.604 recall: 0.468 f1_micro: 0.527 f1_macro: 0.411\n",
      "Time taken: 53.97s\n",
      "Epoch 10\n",
      "Training loss (for one batch) at step 0: 1.6260\n",
      "Training loss (for one batch) at step 50: 1.6889\n",
      "Training loss (for one batch) at step 100: 1.6974\n",
      "Training loss (for one batch) at step 150: 1.6871\n",
      "Validation acc: 0.543 precision: 0.612 recall: 0.469 f1_micro: 0.531 f1_macro: 0.400\n",
      "Time taken: 53.97s\n",
      "Epoch 11\n",
      "Training loss (for one batch) at step 0: 1.7981\n",
      "Training loss (for one batch) at step 50: 1.7709\n",
      "Training loss (for one batch) at step 100: 1.7251\n",
      "Training loss (for one batch) at step 150: 1.6675\n",
      "Validation acc: 0.547 precision: 0.592 recall: 0.487 f1_micro: 0.535 f1_macro: 0.424\n",
      "Time taken: 54.18s\n",
      "Epoch 12\n",
      "Training loss (for one batch) at step 0: 1.6328\n",
      "Training loss (for one batch) at step 50: 1.8138\n",
      "Training loss (for one batch) at step 100: 1.6828\n",
      "Training loss (for one batch) at step 150: 1.9183\n",
      "Validation acc: 0.550 precision: 0.591 recall: 0.490 f1_micro: 0.536 f1_macro: 0.406\n",
      "Time taken: 54.19s\n",
      "Epoch 13\n",
      "Training loss (for one batch) at step 0: 1.4892\n",
      "Training loss (for one batch) at step 50: 1.4635\n",
      "Training loss (for one batch) at step 100: 1.9124\n",
      "Training loss (for one batch) at step 150: 1.4606\n",
      "Validation acc: 0.551 precision: 0.597 recall: 0.494 f1_micro: 0.541 f1_macro: 0.416\n",
      "Time taken: 54.22s\n",
      "Epoch 14\n",
      "Training loss (for one batch) at step 0: 1.7999\n",
      "Training loss (for one batch) at step 50: 1.6960\n",
      "Training loss (for one batch) at step 100: 1.4890\n",
      "Training loss (for one batch) at step 150: 2.1880\n",
      "Validation acc: 0.517 precision: 0.568 recall: 0.431 f1_micro: 0.490 f1_macro: 0.455\n",
      "Time taken: 54.09s\n",
      "Epoch 15\n",
      "Training loss (for one batch) at step 0: 1.8256\n",
      "Training loss (for one batch) at step 50: 1.6187\n",
      "Training loss (for one batch) at step 100: 1.7643\n",
      "Training loss (for one batch) at step 150: 1.7334\n",
      "Validation acc: 0.537 precision: 0.579 recall: 0.473 f1_micro: 0.520 f1_macro: 0.428\n",
      "Time taken: 53.92s\n",
      "Epoch 16\n",
      "Training loss (for one batch) at step 0: 1.7496\n",
      "Training loss (for one batch) at step 50: 1.6062\n",
      "Training loss (for one batch) at step 100: 1.8422\n",
      "Training loss (for one batch) at step 150: 1.6246\n",
      "Validation acc: 0.550 precision: 0.596 recall: 0.471 f1_micro: 0.526 f1_macro: 0.441\n",
      "Time taken: 54.08s\n",
      "Epoch 17\n",
      "Training loss (for one batch) at step 0: 1.7372\n",
      "Training loss (for one batch) at step 50: 1.6467\n",
      "Training loss (for one batch) at step 100: 1.9854\n",
      "Training loss (for one batch) at step 150: 1.6245\n",
      "Validation acc: 0.525 precision: 0.572 recall: 0.467 f1_micro: 0.514 f1_macro: 0.458\n",
      "Time taken: 54.12s\n",
      "Epoch 18\n",
      "Training loss (for one batch) at step 0: 1.7256\n",
      "Training loss (for one batch) at step 50: 1.8889\n",
      "Training loss (for one batch) at step 100: 1.9461\n",
      "Training loss (for one batch) at step 150: 1.7321\n",
      "Validation acc: 0.506 precision: 0.564 recall: 0.424 f1_micro: 0.484 f1_macro: 0.439\n",
      "Time taken: 54.08s\n",
      "Epoch 19\n",
      "Training loss (for one batch) at step 0: 1.9705\n",
      "Training loss (for one batch) at step 50: 1.7630\n",
      "Training loss (for one batch) at step 100: 1.5326\n",
      "Training loss (for one batch) at step 150: 1.5599\n",
      "Validation acc: 0.528 precision: 0.569 recall: 0.452 f1_micro: 0.504 f1_macro: 0.432\n",
      "Time taken: 54.09s\n",
      "Epoch 20\n",
      "Training loss (for one batch) at step 0: 1.6633\n",
      "Training loss (for one batch) at step 50: 1.5155\n",
      "Training loss (for one batch) at step 100: 1.7440\n",
      "Training loss (for one batch) at step 150: 1.7141\n",
      "Validation acc: 0.541 precision: 0.584 recall: 0.488 f1_micro: 0.531 f1_macro: 0.421\n",
      "Time taken: 54.01s\n",
      "Epoch 21\n",
      "Training loss (for one batch) at step 0: 1.7780\n",
      "Training loss (for one batch) at step 50: 1.6418\n",
      "Training loss (for one batch) at step 100: 1.5720\n",
      "Training loss (for one batch) at step 150: 1.7424\n",
      "Validation acc: 0.531 precision: 0.572 recall: 0.484 f1_micro: 0.524 f1_macro: 0.423\n",
      "Time taken: 53.93s\n",
      "Epoch 22\n",
      "Training loss (for one batch) at step 0: 1.5422\n",
      "Training loss (for one batch) at step 50: 1.5258\n",
      "Training loss (for one batch) at step 100: 1.6531\n",
      "Training loss (for one batch) at step 150: 1.6149\n",
      "Validation acc: 0.534 precision: 0.572 recall: 0.469 f1_micro: 0.515 f1_macro: 0.447\n",
      "Time taken: 54.00s\n",
      "Epoch 23\n",
      "Training loss (for one batch) at step 0: 1.8931\n",
      "Training loss (for one batch) at step 50: 2.0037\n",
      "Training loss (for one batch) at step 100: 1.8863\n",
      "Training loss (for one batch) at step 150: 1.7298\n",
      "Validation acc: 0.515 precision: 0.557 recall: 0.454 f1_micro: 0.500 f1_macro: 0.428\n",
      "Time taken: 54.06s\n",
      "Epoch 24\n",
      "Training loss (for one batch) at step 0: 1.9470\n",
      "Training loss (for one batch) at step 50: 1.5159\n",
      "Training loss (for one batch) at step 100: 1.5092\n",
      "Training loss (for one batch) at step 150: 1.8422\n",
      "Validation acc: 0.520 precision: 0.576 recall: 0.455 f1_micro: 0.508 f1_macro: 0.446\n",
      "Time taken: 53.97s\n",
      "Restoring best weights relative to validation accuracy...\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=25\n",
    "max_val_acc = 0.0\n",
    "best_weights = None\n",
    "\n",
    "l_iter = iter(labeled_ds)\n",
    "u_iter = iter(unlabeled_ds)\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f\"Epoch {epoch}\")\n",
    "  start_time = time.time()\n",
    "\n",
    "  for step in range(151):\n",
    "    x, y = next(l_iter)\n",
    "    xu = next(u_iter)\n",
    "    \n",
    "    loss = train_step(x,y,xu,ema_decay)\n",
    "    if step % 50 == 0:\n",
    "      print(\n",
    "        \"Training loss (for one batch) at step %d: %.4f\"\n",
    "        % (step, float(loss))\n",
    "      )\n",
    "  for x_batch_val, y_batch_val in val_ds:\n",
    "      test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "  acc = float(val_acc_metric.result())\n",
    "  prec = float(val_prec_metric.result())\n",
    "  recall = float(val_recall_metric.result())\n",
    "  micro = float(f1_metric_micro.result())\n",
    "  macro = float(f1_metric_macro.result())\n",
    "\n",
    "  val_acc_metric.reset_states()\n",
    "  val_prec_metric.reset_states()\n",
    "  val_recall_metric.reset_states()\n",
    "  f1_metric_micro.reset_states()\n",
    "  f1_metric_macro.reset_states()\n",
    "\n",
    "  if acc > max_val_acc:\n",
    "    max_val_acc = acc\n",
    "    best_weights = model.get_weights()\n",
    "  print(f\"Validation acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")\n",
    "  print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "print(\"Restoring best weights relative to validation accuracy...\")\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.539 precision: 0.593 recall: 0.484 f1_micro: 0.533 f1_macro: 0.385\n"
     ]
    }
   ],
   "source": [
    "for x_batch_val, y_batch_val in test_ds:\n",
    "    test_step(x_batch_val, y_batch_val)\n",
    "acc = float(val_acc_metric.result())\n",
    "prec = float(val_prec_metric.result())\n",
    "recall = float(val_recall_metric.result())\n",
    "micro = float(f1_metric_micro.result())\n",
    "macro = float(f1_metric_macro.result())\n",
    "\n",
    "val_acc_metric.reset_states()\n",
    "val_prec_metric.reset_states()\n",
    "val_recall_metric.reset_states()\n",
    "f1_metric_micro.reset_states()\n",
    "f1_metric_macro.reset_states()\n",
    "print(f\"Test acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

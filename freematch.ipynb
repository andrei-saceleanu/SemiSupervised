{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IS_COLAB = True\n",
    "except:\n",
    "  IS_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "  !pip install tensorflow_addons\n",
    "  !pip install unidecode\n",
    "  !pip install transformers\n",
    "  !pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/SemiSupervised/*.csv ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 12:10:44.714897: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-29 12:10:45.236063: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64::/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:\n",
      "2023-04-29 12:10:45.236109: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64::/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:\n",
      "2023-04-29 12:10:45.236114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-29 12:10:45.877377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 12:10:45.878899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 12:10:45.893441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 12:10:45.894228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 12:10:45.894983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 12:10:45.895732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from unidecode import unidecode\n",
    "from utils import prepare_train_ds, prepare_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 12:10:51.941819: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-29 12:10:51.942459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 12:10:51.942805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 12:10:51.943086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 12:10:52.442414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 12:10:52.442737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 12:10:52.443011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-29 12:10:52.443259: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-04-29 12:10:52.443279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6636 MB memory:  -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "labeled_ds, unlabeled_ds = prepare_train_ds(\"train_ner.csv\",batch_sizel=16,batch_sizeu=16)\n",
    "test_ds = prepare_ds(\"test_ner.csv\")\n",
    "val_ds = prepare_ds(\"validation_internal_ner.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class FreeMatchTune(tf.keras.Model):\n",
    "  def __init__(\n",
    "      self,\n",
    "      encoder_name=\"readerbench/RoBERT-base\",\n",
    "      num_classes=4,\n",
    "      **kwargs\n",
    "  ):\n",
    "    super(FreeMatchTune,self).__init__(**kwargs)\n",
    "\n",
    "    self.bert = TFAutoModel.from_pretrained(encoder_name)\n",
    "    self.num_classes = num_classes\n",
    "    self.weak_augment = tf.keras.layers.GaussianNoise(stddev=0.5)\n",
    "    self.strong_augment = tf.keras.layers.GaussianNoise(stddev=5)\n",
    "\n",
    "    self.cls_head = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(256,activation=\"relu\"),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(self.num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    ids, mask = inputs\n",
    "    \n",
    "    embeds = self.bert(input_ids=ids, attention_mask=mask,training=training).pooler_output\n",
    "\n",
    "    strongs = self.strong_augment(embeds,training=training)\n",
    "    weaks = self.weak_augment(embeds,training=training)\n",
    "\n",
    "    strong_preds = self.cls_head(strongs,training=training)\n",
    "    weak_preds = self.cls_head(weaks,training=training)\n",
    "\n",
    "    return weak_preds, strong_preds\n",
    "  \n",
    "class FreeMatch(tf.keras.Model):\n",
    "  def __init__(\n",
    "      self,\n",
    "      encoder_name=\"readerbench/RoBERT-base\",\n",
    "      num_classes=4,\n",
    "      encoder_train=False,\n",
    "      **kwargs\n",
    "  ):\n",
    "    super(FreeMatch,self).__init__(**kwargs)\n",
    "\n",
    "    self.bert = TFAutoModel.from_pretrained(encoder_name)\n",
    "    self.bert.trainable = encoder_train\n",
    "    self.num_classes = num_classes\n",
    "    self.weak_augment = tf.keras.layers.GaussianNoise(stddev=0.5)\n",
    "    self.strong_augment = tf.keras.layers.GaussianNoise(stddev=5)\n",
    "\n",
    "\n",
    "    self.cls_head = tf.keras.Sequential([\n",
    "        tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(10),\n",
    "            merge_mode=\"ave\"\n",
    "        ),\n",
    "        tf.keras.layers.Dense(units=self.num_classes,activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    ids, mask = inputs\n",
    "    \n",
    "    embeds = self.bert(input_ids=ids, attention_mask=mask).last_hidden_state\n",
    "\n",
    "    strongs = self.strong_augment(embeds,training=training)\n",
    "    weaks = self.weak_augment(embeds,training=training)\n",
    "\n",
    "    strong_preds = self.cls_head(strongs)\n",
    "    weak_preds = self.cls_head(weaks)\n",
    "\n",
    "    return weak_preds, strong_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# model = FreeMatch()\n",
    "model = FreeMatchTune()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "optim = tfa.optimizers.AdamW(weight_decay=0.001,learning_rate=0.005)\n",
    "optim2 = tfa.optimizers.AdamW(weight_decay=0.0,learning_rate=0.00001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_prec_metric = tf.keras.metrics.Precision(name=\"precision\")\n",
    "val_recall_metric = tf.keras.metrics.Recall(name=\"recall\")\n",
    "f1_metric_micro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='micro', name='f1_micro')\n",
    "f1_metric_macro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='macro', name='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "thresh = 0.9\n",
    "unsup_weight = 1.0\n",
    "num_classes = 4\n",
    "fairness_weight = 0.01\n",
    "eps = 1e-7\n",
    "steps_per_epoch = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "tau = tf.Variable(tf.constant(1/num_classes))\n",
    "p_thresh = tf.Variable(tau * tf.ones((num_classes,),dtype=tf.float32))\n",
    "hist_t = tf.Variable(eps * tf.ones((num_classes,),dtype=tf.float32))\n",
    "eps_t = tf.Variable(eps * tf.ones((num_classes,),dtype=tf.float32))\n",
    "\n",
    "ema_decay = tf.constant(0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sumnorm(x):\n",
    "  return x/tf.reduce_sum(x)\n",
    "\n",
    "@tf.function\n",
    "def maxnorm(x):\n",
    "  return x/tf.reduce_max(x)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x,y,xu,ema_decay):\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      wl,_ = model([x[\"input_ids\"],x[\"attention_mask\"]],training=True)\n",
    "      wu,su = model([xu[\"input_ids\"],xu[\"attention_mask\"]],training=True)\n",
    "\n",
    "      ls = loss_fn(y, wl)\n",
    "\n",
    "      local_thresh_update = tf.reduce_mean(tf.reduce_max(wu,axis=1))\n",
    "      tau.assign(ema_decay * tau + (1-ema_decay) * local_thresh_update)\n",
    "      p_thresh.assign(ema_decay * p_thresh + (1-ema_decay) * tf.reduce_mean(wu,axis=0))\n",
    "\n",
    "      hist_t.assign(ema_decay * hist_t + (1-ema_decay) * tf.reduce_sum(tf.one_hot(tf.argmax(wu,axis=1),4),axis=0))\n",
    "      sat = tau * maxnorm(p_thresh)\n",
    "      mask = tf.reduce_max(wu,axis=1)>=tf.gather(sat,tf.argmax(wu,axis=1))\n",
    "      \n",
    "      su = su[mask]\n",
    "      wu = wu[mask]\n",
    "      \n",
    "      lu = loss_fn(tf.argmax(wu,axis=1),su)\n",
    "      \n",
    "      p_avg = tf.reduce_mean(su,axis=0)\n",
    "      h_avg = tf.reduce_sum(tf.one_hot(tf.argmax(su,axis=1),4),axis=0)\n",
    "\n",
    "      lf = -tf.keras.metrics.categorical_crossentropy(sumnorm(p_thresh/hist_t),sumnorm(p_avg/(eps_t+h_avg)))\n",
    "\n",
    "      loss = ls + unsup_weight * lu + fairness_weight * lf\n",
    "\n",
    "  grads = tape.gradient(loss, model.trainable_weights)\n",
    "  optim.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "  return loss\n",
    "\n",
    "@tf.function\n",
    "def train_step_tune(x,y,xu,ema_decay):\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      wl,_ = model([x[\"input_ids\"],x[\"attention_mask\"]],training=True)\n",
    "      wu,su = model([xu[\"input_ids\"],xu[\"attention_mask\"]],training=True)\n",
    "\n",
    "      ls = loss_fn(y, wl)\n",
    "\n",
    "      local_thresh_update = tf.reduce_mean(tf.reduce_max(wu,axis=1))\n",
    "      tau.assign(ema_decay * tau + (1-ema_decay) * local_thresh_update)\n",
    "      p_thresh.assign(ema_decay * p_thresh + (1-ema_decay) * tf.reduce_mean(wu,axis=0))\n",
    "\n",
    "      hist_t.assign(ema_decay * hist_t + (1-ema_decay) * tf.reduce_sum(tf.one_hot(tf.argmax(wu,axis=1),4),axis=0))\n",
    "      sat = tau * maxnorm(p_thresh)\n",
    "      mask = tf.reduce_max(wu,axis=1)>=tf.gather(sat,tf.argmax(wu,axis=1))\n",
    "      \n",
    "      su = su[mask]\n",
    "      wu = wu[mask]\n",
    "      \n",
    "      lu = loss_fn(tf.argmax(wu,axis=1),su)\n",
    "      \n",
    "      lf = 0.0\n",
    "      if tf.shape(su)[0] != 0:\n",
    "        p_avg = tf.reduce_mean(su,axis=0)\n",
    "        h_avg = tf.reduce_sum(tf.one_hot(tf.argmax(su,axis=1),4),axis=0)\n",
    "\n",
    "        lf = -tf.keras.metrics.categorical_crossentropy(sumnorm(p_thresh/hist_t),sumnorm(p_avg/(eps_t+h_avg)))\n",
    "\n",
    "      loss = ls + unsup_weight * lu + fairness_weight * lf\n",
    "\n",
    "  grads = tape.gradient(loss, [model.cls_head.trainable_weights, model.bert.trainable_weights])\n",
    "  optim.apply_gradients(zip(grads[0], model.cls_head.trainable_weights))\n",
    "  optim2.apply_gradients(zip(grads[1], model.bert.trainable_weights))\n",
    "\n",
    "  return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    wpred,_ = model([x[\"input_ids\"],x[\"attention_mask\"]], training=False)\n",
    "    val_acc_metric.update_state(y, wpred)\n",
    "    true_hot = tf.one_hot(y, 4)\n",
    "    val_prec_metric.update_state(true_hot, wpred)\n",
    "    val_recall_metric.update_state(true_hot, wpred)\n",
    "    f1_metric_micro.update_state(true_hot, wpred)\n",
    "    f1_metric_macro.update_state(true_hot, wpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training loss (for one batch) at step 0: 1.5229\n",
      "Training loss (for one batch) at step 50: 2.1581\n",
      "Training loss (for one batch) at step 100: 2.6760\n",
      "Training loss (for one batch) at step 150: 1.3524\n",
      "Training loss (for one batch) at step 200: 1.2020\n",
      "Training loss (for one batch) at step 250: 1.4142\n",
      "Training loss (for one batch) at step 300: 1.7221\n",
      "Training loss (for one batch) at step 350: 1.8700\n",
      "Training loss (for one batch) at step 400: 1.4413\n",
      "Training loss (for one batch) at step 450: 2.1481\n",
      "Training loss (for one batch) at step 500: 1.8101\n",
      "Training loss (for one batch) at step 550: 1.4955\n",
      "Validation acc: 0.459 precision: 0.692 recall: 0.295 f1_micro: 0.414 f1_macro: 0.187\n",
      "Time taken: 230.80s\n",
      "Epoch 1\n",
      "Training loss (for one batch) at step 0: 1.2285\n",
      "Training loss (for one batch) at step 50: 2.1055\n",
      "Training loss (for one batch) at step 100: 4.0407\n",
      "Training loss (for one batch) at step 150: 1.2502\n",
      "Training loss (for one batch) at step 200: 0.7201\n",
      "Training loss (for one batch) at step 250: 0.5326\n",
      "Training loss (for one batch) at step 300: 1.2454\n",
      "Training loss (for one batch) at step 350: 0.7383\n",
      "Training loss (for one batch) at step 400: 0.6262\n",
      "Training loss (for one batch) at step 450: 0.8703\n",
      "Training loss (for one batch) at step 500: 0.7578\n",
      "Training loss (for one batch) at step 550: 0.7994\n",
      "Validation acc: 0.497 precision: 0.813 recall: 0.230 f1_micro: 0.359 f1_macro: 0.180\n",
      "Time taken: 209.36s\n",
      "Epoch 2\n",
      "Training loss (for one batch) at step 0: 0.7684\n",
      "Training loss (for one batch) at step 50: 0.8303\n",
      "Training loss (for one batch) at step 100: 1.3937\n",
      "Training loss (for one batch) at step 150: 0.9698\n",
      "Training loss (for one batch) at step 200: 1.4696\n",
      "Training loss (for one batch) at step 250: 3.8723\n",
      "Training loss (for one batch) at step 300: 1.9348\n",
      "Training loss (for one batch) at step 350: 1.6559\n",
      "Training loss (for one batch) at step 400: 0.8276\n",
      "Training loss (for one batch) at step 450: 2.2772\n",
      "Training loss (for one batch) at step 500: 2.4742\n",
      "Training loss (for one batch) at step 550: 0.7453\n",
      "Validation acc: 0.525 precision: 0.647 recall: 0.317 f1_micro: 0.426 f1_macro: 0.186\n",
      "Time taken: 209.51s\n",
      "Epoch 3\n",
      "Training loss (for one batch) at step 0: 1.2688\n",
      "Training loss (for one batch) at step 50: 0.7267\n",
      "Training loss (for one batch) at step 100: 0.7502\n",
      "Training loss (for one batch) at step 150: 1.0296\n",
      "Training loss (for one batch) at step 200: 0.6211\n",
      "Training loss (for one batch) at step 250: 0.8973\n",
      "Training loss (for one batch) at step 300: 0.3625\n",
      "Training loss (for one batch) at step 350: 0.5236\n",
      "Training loss (for one batch) at step 400: 0.5268\n",
      "Training loss (for one batch) at step 450: 1.3256\n",
      "Training loss (for one batch) at step 500: 0.9151\n",
      "Training loss (for one batch) at step 550: 0.6546\n",
      "Validation acc: 0.414 precision: 0.503 recall: 0.359 f1_micro: 0.419 f1_macro: 0.263\n",
      "Time taken: 209.28s\n",
      "Epoch 4\n",
      "Training loss (for one batch) at step 0: 1.1078\n",
      "Training loss (for one batch) at step 50: 1.0794\n",
      "Training loss (for one batch) at step 100: 0.9443\n",
      "Training loss (for one batch) at step 150: 1.9715\n",
      "Training loss (for one batch) at step 200: 0.9155\n",
      "Training loss (for one batch) at step 250: 0.4858\n",
      "Training loss (for one batch) at step 300: 0.5600\n",
      "Training loss (for one batch) at step 350: 0.3447\n",
      "Training loss (for one batch) at step 400: 0.7468\n",
      "Training loss (for one batch) at step 450: 0.5038\n",
      "Training loss (for one batch) at step 500: 0.8445\n",
      "Training loss (for one batch) at step 550: 0.8207\n",
      "Validation acc: 0.540 precision: 0.543 recall: 0.537 f1_micro: 0.540 f1_macro: 0.332\n",
      "Time taken: 209.37s\n",
      "Epoch 5\n",
      "Training loss (for one batch) at step 0: 1.0625\n",
      "Training loss (for one batch) at step 50: 0.3516\n",
      "Training loss (for one batch) at step 100: 0.6382\n",
      "Training loss (for one batch) at step 150: 0.5807\n",
      "Training loss (for one batch) at step 200: 0.2338\n",
      "Training loss (for one batch) at step 250: 0.6478\n",
      "Training loss (for one batch) at step 300: 0.3330\n",
      "Training loss (for one batch) at step 350: 0.8877\n",
      "Training loss (for one batch) at step 400: 0.4332\n",
      "Training loss (for one batch) at step 450: 0.5246\n",
      "Training loss (for one batch) at step 500: 1.4419\n",
      "Training loss (for one batch) at step 550: 0.5920\n",
      "Validation acc: 0.502 precision: 0.805 recall: 0.239 f1_micro: 0.369 f1_macro: 0.183\n",
      "Time taken: 209.28s\n",
      "Epoch 6\n",
      "Training loss (for one batch) at step 0: 0.5427\n",
      "Training loss (for one batch) at step 50: 0.8369\n",
      "Training loss (for one batch) at step 100: 1.9367\n",
      "Training loss (for one batch) at step 150: 0.6553\n",
      "Training loss (for one batch) at step 200: 6.0616\n",
      "Training loss (for one batch) at step 250: 0.7500\n",
      "Training loss (for one batch) at step 300: 0.4383\n",
      "Training loss (for one batch) at step 350: 0.7961\n",
      "Training loss (for one batch) at step 400: 1.0298\n",
      "Training loss (for one batch) at step 450: 0.3791\n",
      "Training loss (for one batch) at step 500: 0.8490\n",
      "Training loss (for one batch) at step 550: 0.6470\n",
      "Validation acc: 0.524 precision: 0.744 recall: 0.273 f1_micro: 0.400 f1_macro: 0.195\n",
      "Time taken: 209.25s\n",
      "Epoch 7\n",
      "Training loss (for one batch) at step 0: 0.5673\n",
      "Training loss (for one batch) at step 50: 1.4511\n",
      "Training loss (for one batch) at step 100: 1.3293\n",
      "Training loss (for one batch) at step 150: 0.4866\n",
      "Training loss (for one batch) at step 200: 0.5366\n",
      "Training loss (for one batch) at step 250: 0.5326\n",
      "Training loss (for one batch) at step 300: 0.5806\n",
      "Training loss (for one batch) at step 350: 0.6963\n",
      "Training loss (for one batch) at step 400: 0.9050\n",
      "Training loss (for one batch) at step 450: 0.6938\n",
      "Training loss (for one batch) at step 500: 0.6967\n",
      "Training loss (for one batch) at step 550: 0.5148\n",
      "Validation acc: 0.471 precision: 0.746 recall: 0.256 f1_micro: 0.381 f1_macro: 0.209\n",
      "Time taken: 209.21s\n",
      "Epoch 8\n",
      "Training loss (for one batch) at step 0: 0.9602\n",
      "Training loss (for one batch) at step 50: 0.9640\n",
      "Training loss (for one batch) at step 100: 0.2986\n",
      "Training loss (for one batch) at step 150: 0.4808\n",
      "Training loss (for one batch) at step 200: 0.7170\n",
      "Training loss (for one batch) at step 250: 0.5082\n",
      "Training loss (for one batch) at step 300: 0.8480\n",
      "Training loss (for one batch) at step 350: 4.0162\n",
      "Training loss (for one batch) at step 400: 1.2188\n",
      "Training loss (for one batch) at step 450: 0.7968\n",
      "Training loss (for one batch) at step 500: 0.5777\n",
      "Training loss (for one batch) at step 550: 0.5833\n",
      "Validation acc: 0.474 precision: 0.820 recall: 0.206 f1_micro: 0.329 f1_macro: 0.167\n",
      "Time taken: 209.14s\n",
      "Epoch 9\n",
      "Training loss (for one batch) at step 0: 0.4810\n",
      "Training loss (for one batch) at step 50: 4.7928\n",
      "Training loss (for one batch) at step 100: 1.0423\n",
      "Training loss (for one batch) at step 150: 1.3564\n",
      "Training loss (for one batch) at step 200: 5.5544\n",
      "Training loss (for one batch) at step 250: 0.6012\n",
      "Training loss (for one batch) at step 300: 2.8966\n",
      "Training loss (for one batch) at step 350: 1.9090\n",
      "Training loss (for one batch) at step 400: 1.1072\n",
      "Training loss (for one batch) at step 450: 2.4225\n",
      "Training loss (for one batch) at step 500: 1.7333\n",
      "Training loss (for one batch) at step 550: 1.6632\n",
      "Validation acc: 0.161 precision: 0.862 recall: 0.056 f1_micro: 0.106 f1_macro: 0.119\n",
      "Time taken: 209.30s\n",
      "Epoch 10\n",
      "Training loss (for one batch) at step 0: 1.2653\n",
      "Training loss (for one batch) at step 50: 0.7272\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(l_iter)\n\u001b[1;32m     13\u001b[0m xu \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(u_iter)\n\u001b[0;32m---> 15\u001b[0m loss \u001b[39m=\u001b[39m train_step_tune(x,y,xu,ema_decay)\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     17\u001b[0m   \u001b[39mprint\u001b[39m(\n\u001b[1;32m     18\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTraining loss (for one batch) at step \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[39m%\u001b[39m (step, \u001b[39mfloat\u001b[39m(loss))\n\u001b[1;32m     20\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SSL-vhLnKE8Q/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SSL-vhLnKE8Q/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SSL-vhLnKE8Q/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SSL-vhLnKE8Q/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SSL-vhLnKE8Q/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SSL-vhLnKE8Q/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SSL-vhLnKE8Q/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS=15\n",
    "max_val_acc = 0.0\n",
    "best_weights = None\n",
    "\n",
    "l_iter = iter(labeled_ds)\n",
    "u_iter = iter(unlabeled_ds)\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f\"Epoch {epoch}\")\n",
    "  start_time = time.time()\n",
    "\n",
    "  for step in range(steps_per_epoch):\n",
    "    x, y = next(l_iter)\n",
    "    xu = next(u_iter)\n",
    "    \n",
    "    loss = train_step_tune(x,y,xu,ema_decay)\n",
    "    if step % 50 == 0:\n",
    "      print(\n",
    "        \"Training loss (for one batch) at step %d: %.4f\"\n",
    "        % (step, float(loss))\n",
    "      )\n",
    "  for x_batch_val, y_batch_val in val_ds:\n",
    "      test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "  acc = float(val_acc_metric.result())\n",
    "  prec = float(val_prec_metric.result())\n",
    "  recall = float(val_recall_metric.result())\n",
    "  micro = float(f1_metric_micro.result())\n",
    "  macro = float(f1_metric_macro.result())\n",
    "\n",
    "  val_acc_metric.reset_states()\n",
    "  val_prec_metric.reset_states()\n",
    "  val_recall_metric.reset_states()\n",
    "  f1_metric_micro.reset_states()\n",
    "  f1_metric_macro.reset_states()\n",
    "\n",
    "  if acc > max_val_acc:\n",
    "    max_val_acc = acc\n",
    "    best_weights = model.get_weights()\n",
    "  print(f\"Validation acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")\n",
    "  print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "print(\"Restoring best weights relative to validation accuracy...\")\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.562 precision: 0.610 recall: 0.485 f1_micro: 0.540 f1_macro: 0.402\n"
     ]
    }
   ],
   "source": [
    "for x_batch_val, y_batch_val in test_ds:\n",
    "    test_step(x_batch_val, y_batch_val)\n",
    "acc = float(val_acc_metric.result())\n",
    "prec = float(val_prec_metric.result())\n",
    "recall = float(val_recall_metric.result())\n",
    "micro = float(f1_metric_micro.result())\n",
    "macro = float(f1_metric_macro.result())\n",
    "\n",
    "val_acc_metric.reset_states()\n",
    "val_prec_metric.reset_states()\n",
    "val_recall_metric.reset_states()\n",
    "f1_metric_micro.reset_states()\n",
    "f1_metric_macro.reset_states()\n",
    "print(f\"Test acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model.cls_head.save_weights(\"./checkpoints/freematch_notune\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq6Z0l4bnXdf"
      },
      "source": [
        "## Google colab setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWzIteSZX3v6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IS_COLAB = True\n",
        "except:\n",
        "  IS_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL44kjMDyd2K",
        "outputId": "f5132138-a441-4131-ff8d-295e4e91df24",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow_addons) (23.0)\n",
            "Collecting typeguard>=2.7\n",
            "  Downloading typeguard-3.0.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow_addons) (6.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow_addons) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.7->tensorflow_addons) (3.15.0)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.19.0 typeguard-3.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (4.6.4)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (3.10.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.26.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ],
      "source": [
        "if IS_COLAB:\n",
        "  !pip install tensorflow_addons\n",
        "  !pip install unidecode\n",
        "  !pip install transformers\n",
        "  !pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2tHZhicYCx5",
        "outputId": "28bbfe6b-a93d-4faa-85a9-892349ab1980",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbZGcnvfYDek",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/SemiSupervised/*.csv ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LnA0eoanXdt"
      },
      "source": [
        "## Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0aKiJSs9wEIK",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-24 13:42:41.738072: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-24 13:42:42.250576: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64::/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:\n",
            "2023-03-24 13:42:42.250621: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64::/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:\n",
            "2023-03-24 13:42:42.250626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-24 13:42:42.899394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-24 13:42:42.899730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-24 13:42:42.903395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-24 13:42:42.903691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-24 13:42:42.903976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-24 13:42:42.904257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import spacy\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ks_2wS0HnXdu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "tf.config.set_visible_devices(gpus[0], 'GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k3oBZdDZdYt"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rgLCfdptY3Qi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"train_ner.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HEF79yRo_CAN",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def preprocess(x):\n",
        "    s = unidecode(x)\n",
        "    s = str.lower(s)\n",
        "    s = re.sub(r\"\\[[a-z]+\\]\",\"\", s)\n",
        "    s = re.sub(r\"\\*\",\"\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z0-9]+\",\" \",s)\n",
        "    s = re.sub(r\" +\",\" \",s)\n",
        "    s = re.sub(r\"(.)\\1+\",r\"\\1\",s)\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQrrio17pNjI",
        "outputId": "49e1466d-afc2-4784-aede-ee09eb0be060",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ABUSE': 0, 'INSULT': 1, 'OTHER': 2, 'PROFANITY': 3}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_ids = {label_name:i for i, label_name in enumerate(sorted(set(data[\"label\"])))}\n",
        "label_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VVCrVZFQDddt",
        "outputId": "db2ab8ed-251c-44b4-d4d9-63d4a39ef238",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32674</td>\n",
              "      <td>da de unde stii u mai [ORG] ca banii au fost p...</td>\n",
              "      <td>ABUSE</td>\n",
              "      <td>da de unde sti u mai ca bani au fost pt si nu ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16514</td>\n",
              "      <td>m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...</td>\n",
              "      <td>INSULT</td>\n",
              "      <td>muie muie muie bai kakatule stai in banca ta d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32556</td>\n",
              "      <td>PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>pt ala care are treaba cu esti unul care nu ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23861</td>\n",
              "      <td>sunt bucuros ca [PERS] nu a mai venit la [ORG]...</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>sunt bucuros ca nu a mai venit la jucator de d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21811</td>\n",
              "      <td>[PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...</td>\n",
              "      <td>INSULT</td>\n",
              "      <td>esti pe n es cu li bi l te asemeni cu de la c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                               text   label  \\\n",
              "0  32674  da de unde stii u mai [ORG] ca banii au fost p...   ABUSE   \n",
              "1  16514  m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...  INSULT   \n",
              "2  32556  PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...   OTHER   \n",
              "3  23861  sunt bucuros ca [PERS] nu a mai venit la [ORG]...   OTHER   \n",
              "4  21811  [PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...  INSULT   \n",
              "\n",
              "                                        preprocessed  \n",
              "0  da de unde sti u mai ca bani au fost pt si nu ...  \n",
              "1  muie muie muie bai kakatule stai in banca ta d...  \n",
              "2  pt ala care are treaba cu esti unul care nu ar...  \n",
              "3  sunt bucuros ca nu a mai venit la jucator de d...  \n",
              "4   esti pe n es cu li bi l te asemeni cu de la c...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"preprocessed\"] = data[\"text\"].apply(preprocess)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MGMYw4qJpNjM",
        "outputId": "f3932033-18a9-4853-e7db-b59d8f824652",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>preprocessed</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32674</td>\n",
              "      <td>da de unde stii u mai [ORG] ca banii au fost p...</td>\n",
              "      <td>ABUSE</td>\n",
              "      <td>da de unde sti u mai ca bani au fost pt si nu ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16514</td>\n",
              "      <td>m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...</td>\n",
              "      <td>INSULT</td>\n",
              "      <td>muie muie muie bai kakatule stai in banca ta d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32556</td>\n",
              "      <td>PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>pt ala care are treaba cu esti unul care nu ar...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23861</td>\n",
              "      <td>sunt bucuros ca [PERS] nu a mai venit la [ORG]...</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>sunt bucuros ca nu a mai venit la jucator de d...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21811</td>\n",
              "      <td>[PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...</td>\n",
              "      <td>INSULT</td>\n",
              "      <td>esti pe n es cu li bi l te asemeni cu de la c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                               text   label  \\\n",
              "0  32674  da de unde stii u mai [ORG] ca banii au fost p...   ABUSE   \n",
              "1  16514  m*uie [PERS] m*uie [PERS] ... m*uie\\nbai kakat...  INSULT   \n",
              "2  32556  PT ALA CARE ARE TREABA CU [PERS]!!ESTI UNUL CA...   OTHER   \n",
              "3  23861  sunt bucuros ca [PERS] nu a mai venit la [ORG]...   OTHER   \n",
              "4  21811  [PERS] esti....PE..N\\n ES..CU..LI..BI.L!!! te ...  INSULT   \n",
              "\n",
              "                                        preprocessed  class  \n",
              "0  da de unde sti u mai ca bani au fost pt si nu ...      0  \n",
              "1  muie muie muie bai kakatule stai in banca ta d...      1  \n",
              "2  pt ala care are treaba cu esti unul care nu ar...      2  \n",
              "3  sunt bucuros ca nu a mai venit la jucator de d...      2  \n",
              "4   esti pe n es cu li bi l te asemeni cu de la c...      1  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"class\"] = data[\"label\"].map(lambda x: label_ids[x])\n",
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4y5CV_Qd_Mm",
        "outputId": "5babecb4-cab2-4556-db58-088fd47dad01",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OTHER        3649\n",
              "ABUSE        2768\n",
              "INSULT       2242\n",
              "PROFANITY    1294\n",
              "Name: label, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz3Oxo-jnXdz"
      },
      "source": [
        "## Prepare data splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bwkEdWnQtJXt",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-03-24 13:43:05.221248: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-24 13:43:05.221877: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-24 13:43:05.222219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-24 13:43:05.222502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-24 13:43:05.721289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-24 13:43:05.721607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-24 13:43:05.721942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2023-03-24 13:43:05.722186: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-03-24 13:43:05.722206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6636 MB memory:  -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
          ]
        }
      ],
      "source": [
        "tok_robert = AutoTokenizer.from_pretrained(\"readerbench/RoBERT-base\")\n",
        "robert = TFAutoModel.from_pretrained(\"readerbench/RoBERT-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7YeG8Va3e4X9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def split_ssl_data(ids_array,mask_array,labels,num_classes,label_percent):\n",
        "  labeled = None\n",
        "  unlabeled = None\n",
        "\n",
        "  for class_idx in range(num_classes):\n",
        "    class_ids = ids_array[labels==class_idx]\n",
        "    class_mask = mask_array[labels==class_idx]\n",
        "    sz = int(label_percent * class_ids.shape[0])\n",
        "\n",
        "    labels_reduced = labels[labels==class_idx][:sz]\n",
        "    labeled_ids, unlabeled_ids = class_ids[:sz], class_ids[sz:]\n",
        "    labeled_mask, unlabeled_mask = class_mask[:sz], class_mask[sz:]\n",
        "\n",
        "    if not labeled:\n",
        "      labeled = (labeled_ids, labeled_mask, labels_reduced)\n",
        "      unlabeled = (unlabeled_ids, unlabeled_mask)\n",
        "    else:\n",
        "      labeled = (\n",
        "          np.concatenate([labeled[0],labeled_ids]),\n",
        "          np.concatenate([labeled[1],labeled_mask]),\n",
        "          np.concatenate([labeled[2],labels_reduced])\n",
        "      )\n",
        "      unlabeled = (\n",
        "          np.concatenate([unlabeled[0],unlabeled_ids]),\n",
        "          np.concatenate([unlabeled[1],unlabeled_mask]),\n",
        "      )\n",
        "\n",
        "  return labeled, unlabeled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XgvnKQemZENV",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def preprocess_robert(x):\n",
        "  t = tok_robert(x,padding=\"max_length\",max_length=96,truncation=True,return_tensors='np')\n",
        "  return t[\"input_ids\"], t[\"attention_mask\"]\n",
        "\n",
        "def map_func(input_ids, masks, labels):\n",
        "  return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
        "\n",
        "def map_func2(input_ids, masks):\n",
        "  return {'input_ids': input_ids, 'attention_mask': masks}\n",
        "\n",
        "def prepare_ds(filename,batch_size=64,label_percent=0.8):\n",
        "  df = pd.read_csv(filename)\n",
        "  X_id_mask = df['text'].map(preprocess).apply(preprocess_robert).apply(pd.Series)\n",
        "\n",
        "  X_id_mask.columns = [\"input_ids\",\"attention_mask\"]\n",
        "\n",
        "  ids_array = np.squeeze(np.stack(X_id_mask.input_ids.values), axis=1)\n",
        "  mask_array = np.squeeze(np.stack(X_id_mask.attention_mask.values), axis=1)\n",
        "  labels = df[\"label\"].map(lambda x: label_ids[x]).values\n",
        "\n",
        "  res_ds = tf.data.Dataset.from_tensor_slices((ids_array, mask_array, labels)).map(map_func).shuffle(len(df)).batch(batch_size)\n",
        "  return res_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-GLbjR7PfbrL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def prepare_train_ds(filename,batch_size=16,label_percent=0.05):\n",
        "  df = pd.read_csv(filename)\n",
        "  df = df.sample(frac=1)\n",
        "  X_id_mask = df['text'].map(preprocess).apply(preprocess_robert).apply(pd.Series)\n",
        "\n",
        "  X_id_mask.columns = [\"input_ids\",\"attention_mask\"]\n",
        "\n",
        "  ids_array = np.squeeze(np.stack(X_id_mask.input_ids.values), axis=1)\n",
        "  mask_array = np.squeeze(np.stack(X_id_mask.attention_mask.values), axis=1)\n",
        "  labels = df[\"label\"].map(lambda x: label_ids[x]).values\n",
        "\n",
        "  labeled, unlabeled = split_ssl_data(ids_array,mask_array,labels,len(label_ids),label_percent)\n",
        "  lres = None\n",
        "  ures = None\n",
        "  ids,mask,labels = labeled\n",
        "  for i in range(0,len(ids),200):\n",
        "    end = min(i+200,len(ids))\n",
        "    embeds = robert(input_ids=ids[i:end],attention_mask=mask[i:end],training=False).last_hidden_state.numpy()\n",
        "    if i==0:\n",
        "      lres = embeds\n",
        "    else:\n",
        "      lres = np.concatenate([lres,embeds])\n",
        "  ids,mask = unlabeled\n",
        "  for i in range(0,len(ids),200):\n",
        "      end = min(i+200,len(ids))\n",
        "      embeds = robert(input_ids=ids[i:end],attention_mask=mask[i:end],training=False).last_hidden_state.numpy()\n",
        "      if i==0:\n",
        "        ures = embeds\n",
        "      else:\n",
        "        ures = np.concatenate([ures,embeds])\n",
        "\n",
        "  labeled_ds = tf.data.Dataset.from_tensor_slices((lres,labels))\n",
        "  labeled_ds = labeled_ds.shuffle(len(labeled_ds)).batch(batch_size).repeat()\n",
        "  unlabeled_ds = tf.data.Dataset.from_tensor_slices(ures)\n",
        "  unlabeled_ds = unlabeled_ds.shuffle(len(unlabeled_ds)).batch(4*batch_size).repeat()\n",
        "\n",
        "  return labeled_ds, unlabeled_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "thQeS_wKZFe7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "labeled_ds, unlabeled_ds= prepare_train_ds(\"train_ner.csv\")\n",
        "test_ds = prepare_ds(\"test_ner.csv\")\n",
        "val_ds = prepare_ds(\"validation_internal_ner.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5scWgSxY8fd"
      },
      "source": [
        "## Model definition and declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2xa0RNQotX5E",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class MixMatch(tf.keras.Model):\n",
        "  def __init__(self,num_classes=4,**kwargs):\n",
        "    super(MixMatch, self).__init__(**kwargs)\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    self.cls_head = tf.keras.Sequential([\n",
        "        tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(10,dropout=0.2),\n",
        "            merge_mode=\"ave\"\n",
        "        ),\n",
        "        tf.keras.layers.Dense(units=self.num_classes,activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    \n",
        "    out = self.cls_head(inputs,training=training)\n",
        "    return out\n",
        "  \n",
        "class Augment(tf.keras.Model):\n",
        "  def __init__(self,**kwargs):\n",
        "    super(Augment,self).__init__(**kwargs)\n",
        "    self.augment = tf.keras.layers.GaussianNoise(stddev=1)\n",
        "\n",
        "  def call(self,inputs,training):\n",
        "    return self.augment(inputs,training=training)\n",
        "\n",
        "class BERTEncoder(tf.keras.Model):\n",
        "  def __init__(self,bert_model,**kwargs):\n",
        "    super(BERTEncoder, self).__init__(**kwargs)\n",
        "    self.bert = bert_model\n",
        "    self.bert.trainable = False\n",
        "    \n",
        "\n",
        "  def call(self, inputs):\n",
        "    ids, mask = inputs\n",
        "    embeds = self.bert(input_ids=ids, attention_mask=mask).last_hidden_state\n",
        "    return embeds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OID_8kmty4t3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "cls_head = MixMatch()\n",
        "encoder = BERTEncoder(bert_model=robert)\n",
        "aug = Augment()\n",
        "ids_input = tf.keras.Input(shape=(96,),dtype=tf.int32)\n",
        "mask_input = tf.keras.Input(shape=(96,),dtype=tf.int32)\n",
        "embeds = encoder([ids_input,mask_input])\n",
        "model_output = cls_head(aug(embeds))\n",
        "model = tf.keras.Model(inputs=[ids_input,mask_input],outputs=model_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6T0TmNvnXd4"
      },
      "source": [
        "## Train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1Lf5lS1DzZER",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "optim = tfa.optimizers.AdamW(weight_decay=0.001,learning_rate=0.001)\n",
        "entropy = tf.keras.losses.CategoricalCrossentropy()\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "val_prec_metric = tf.keras.metrics.Precision(name=\"precision\")\n",
        "val_recall_metric = tf.keras.metrics.Recall(name=\"recall\")\n",
        "f1_metric_micro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='micro', name='f1_micro')\n",
        "f1_metric_macro = tfa.metrics.F1Score(num_classes=4, threshold=0.5, average='macro', name='f1_macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DvdF7Al_puI_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def weight_schedule(t, init_period, final_period, max_period):\n",
        "\n",
        "  if t < init_period:\n",
        "    x = t / init_period\n",
        "    return 2.71 ** (-5*(x-1)**2)\n",
        "  elif t < max_period - final_period:\n",
        "    return 1.0\n",
        "  else:\n",
        "    x = (t - max_period + final_period) / final_period\n",
        "    return 1 - 2.71 ** (-5*(x-1)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HupoVbzTYiFA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "epochs = 30\n",
        "K = 2\n",
        "steps_per_epoch = 151\n",
        "max_period = epochs * steps_per_epoch\n",
        "alpha = 10\n",
        "unsup_weight = tf.Variable(1.0)\n",
        "init_period = 700\n",
        "final_period = 700\n",
        "blend = tf.constant(1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6tglQyM0poY6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def sharpen(x,T=0.5):\n",
        "  power = 1/T\n",
        "  x = x**power\n",
        "  s = tf.expand_dims(tf.reduce_sum(x,axis=1),axis=-1)\n",
        "  return x/s\n",
        "\n",
        "@tf.function\n",
        "def train_step(x,y,xu,unsup_weight,blend):\n",
        "\n",
        "  x_aug = aug(x,training=True)\n",
        "\n",
        "  r_xu = tf.repeat(xu,K,axis=0)\n",
        "  cnt = tf.shape(x_aug)[0]\n",
        "\n",
        "  xu_aug = aug(r_xu, training=True)\n",
        "\n",
        "  pred_unsup = cls_head(xu_aug,training=False)\n",
        "\n",
        "  pred_unsup = tf.reshape(pred_unsup,(-1, K, 4))\n",
        "\n",
        "  pred_unsup = tf.repeat(sharpen(tf.reduce_mean(pred_unsup,axis=1)),K,axis=0)\n",
        "\n",
        "  y_ext = tf.one_hot(y,4)\n",
        "\n",
        "# (x_aug, y_ext) , (xu_aug, pred_unsup)\n",
        "\n",
        "  x_total = tf.random.shuffle(tf.concat([x_aug,xu_aug],axis=0),seed=1)\n",
        "  total = tf.shape(x_total)[0]\n",
        "  y_total = tf.random.shuffle(tf.concat([y_ext, pred_unsup],axis=0),seed=1)\n",
        "\n",
        "  first = tf.gather(x_total,range(cnt),axis=0)\n",
        "  first_labels = tf.gather(y_total,range(cnt),axis=0)\n",
        "\n",
        "\n",
        "  second = tf.gather(x_total,range(cnt,total),axis=0)\n",
        "  second_labels = tf.gather(y_total,range(cnt,total),axis=0)\n",
        "\n",
        "  a = blend * x_aug + (1-blend) * first\n",
        "  a_target = blend * y_ext + (1-blend) * first_labels\n",
        "\n",
        "  b = blend * xu_aug + (1-blend) * second\n",
        "  b_target = blend * pred_unsup + (1-blend) * second_labels\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      \n",
        "      a_pred = cls_head(a,training=True)\n",
        "      b_pred = cls_head(b,training=True)\n",
        "\n",
        "      loss = entropy(a_target, a_pred) + unsup_weight * mse(b_target, b_pred) / 4\n",
        "      \n",
        "  grads = tape.gradient(loss, cls_head.trainable_weights)\n",
        "  optim.apply_gradients(zip(grads, cls_head.trainable_weights))\n",
        "\n",
        "  return loss\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    wpred = model([x[\"input_ids\"],x[\"attention_mask\"]], training=False)\n",
        "    val_acc_metric.update_state(y, wpred)\n",
        "    true_hot = tf.one_hot(y, 4)\n",
        "    val_prec_metric.update_state(true_hot, wpred)\n",
        "    val_recall_metric.update_state(true_hot, wpred)\n",
        "    f1_metric_micro.update_state(true_hot, wpred)\n",
        "    f1_metric_macro.update_state(true_hot, wpred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKGNoBX2hdZ6",
        "outputId": "7a440f55-2149-4f4a-a975-1152cffeba8a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Training loss (for one batch) at step 0: 1.3761\n",
            "Training loss (for one batch) at step 50: 1.1941\n",
            "Training loss (for one batch) at step 100: 1.1769\n",
            "Training loss (for one batch) at step 150: 1.1775\n",
            "Validation acc: 0.492 precision: 0.629 recall: 0.260 f1_micro: 0.368 f1_macro: 0.172\n",
            "Time taken: 23.72s\n",
            "Epoch 1\n",
            "Training loss (for one batch) at step 0: 1.1100\n",
            "Training loss (for one batch) at step 50: 1.2021\n",
            "Training loss (for one batch) at step 100: 1.1366\n",
            "Training loss (for one batch) at step 150: 1.1749\n",
            "Validation acc: 0.528 precision: 0.623 recall: 0.398 f1_micro: 0.486 f1_macro: 0.303\n",
            "Time taken: 16.04s\n",
            "Epoch 2\n",
            "Training loss (for one batch) at step 0: 1.1373\n",
            "Training loss (for one batch) at step 50: 1.0992\n",
            "Training loss (for one batch) at step 100: 0.9356\n",
            "Training loss (for one batch) at step 150: 1.2303\n",
            "Validation acc: 0.529 precision: 0.628 recall: 0.414 f1_micro: 0.499 f1_macro: 0.318\n",
            "Time taken: 16.69s\n",
            "Epoch 3\n",
            "Training loss (for one batch) at step 0: 1.1138\n",
            "Training loss (for one batch) at step 50: 1.0296\n",
            "Training loss (for one batch) at step 100: 1.1236\n",
            "Training loss (for one batch) at step 150: 1.1518\n",
            "Validation acc: 0.532 precision: 0.614 recall: 0.415 f1_micro: 0.496 f1_macro: 0.304\n",
            "Time taken: 16.79s\n",
            "Epoch 4\n",
            "Training loss (for one batch) at step 0: 1.0643\n",
            "Training loss (for one batch) at step 50: 0.9690\n",
            "Training loss (for one batch) at step 100: 1.0169\n",
            "Training loss (for one batch) at step 150: 1.1562\n",
            "Validation acc: 0.561 precision: 0.672 recall: 0.395 f1_micro: 0.498 f1_macro: 0.354\n",
            "Time taken: 17.56s\n",
            "Epoch 5\n",
            "Training loss (for one batch) at step 0: 1.2563\n",
            "Training loss (for one batch) at step 50: 1.2130\n",
            "Training loss (for one batch) at step 100: 1.1165\n",
            "Training loss (for one batch) at step 150: 0.9224\n",
            "Validation acc: 0.538 precision: 0.611 recall: 0.448 f1_micro: 0.517 f1_macro: 0.337\n",
            "Time taken: 16.11s\n",
            "Epoch 6\n",
            "Training loss (for one batch) at step 0: 1.0366\n",
            "Training loss (for one batch) at step 50: 1.1415\n",
            "Training loss (for one batch) at step 100: 1.1201\n",
            "Training loss (for one batch) at step 150: 0.9867\n",
            "Validation acc: 0.543 precision: 0.633 recall: 0.412 f1_micro: 0.499 f1_macro: 0.334\n",
            "Time taken: 16.04s\n",
            "Epoch 7\n",
            "Training loss (for one batch) at step 0: 0.9425\n",
            "Training loss (for one batch) at step 50: 0.8316\n",
            "Training loss (for one batch) at step 100: 1.1486\n",
            "Training loss (for one batch) at step 150: 1.0511\n",
            "Validation acc: 0.549 precision: 0.637 recall: 0.413 f1_micro: 0.502 f1_macro: 0.352\n",
            "Time taken: 16.23s\n",
            "Epoch 8\n",
            "Training loss (for one batch) at step 0: 1.1705\n",
            "Training loss (for one batch) at step 50: 1.1166\n",
            "Training loss (for one batch) at step 100: 0.9564\n",
            "Training loss (for one batch) at step 150: 1.1168\n",
            "Validation acc: 0.543 precision: 0.655 recall: 0.380 f1_micro: 0.481 f1_macro: 0.349\n",
            "Time taken: 15.86s\n",
            "Epoch 9\n",
            "Training loss (for one batch) at step 0: 0.9910\n",
            "Training loss (for one batch) at step 50: 0.9289\n",
            "Training loss (for one batch) at step 100: 1.0080\n",
            "Training loss (for one batch) at step 150: 0.9444\n",
            "Validation acc: 0.563 precision: 0.669 recall: 0.372 f1_micro: 0.478 f1_macro: 0.363\n",
            "Time taken: 15.88s\n",
            "Epoch 10\n",
            "Training loss (for one batch) at step 0: 1.0672\n",
            "Training loss (for one batch) at step 50: 1.0255\n",
            "Training loss (for one batch) at step 100: 0.9590\n",
            "Training loss (for one batch) at step 150: 0.9090\n",
            "Validation acc: 0.529 precision: 0.659 recall: 0.347 f1_micro: 0.454 f1_macro: 0.320\n",
            "Time taken: 15.59s\n",
            "Epoch 11\n",
            "Training loss (for one batch) at step 0: 1.0461\n",
            "Training loss (for one batch) at step 50: 0.9716\n",
            "Training loss (for one batch) at step 100: 0.9277\n",
            "Training loss (for one batch) at step 150: 0.9860\n",
            "Validation acc: 0.559 precision: 0.638 recall: 0.455 f1_micro: 0.531 f1_macro: 0.371\n",
            "Time taken: 15.79s\n",
            "Epoch 12\n",
            "Training loss (for one batch) at step 0: 0.8924\n",
            "Training loss (for one batch) at step 50: 1.0837\n",
            "Training loss (for one batch) at step 100: 1.1052\n",
            "Training loss (for one batch) at step 150: 1.0057\n",
            "Validation acc: 0.533 precision: 0.629 recall: 0.411 f1_micro: 0.497 f1_macro: 0.372\n",
            "Time taken: 16.50s\n",
            "Epoch 13\n",
            "Training loss (for one batch) at step 0: 1.1083\n",
            "Training loss (for one batch) at step 50: 0.9559\n",
            "Training loss (for one batch) at step 100: 0.8905\n",
            "Training loss (for one batch) at step 150: 0.9345\n",
            "Validation acc: 0.555 precision: 0.653 recall: 0.352 f1_micro: 0.458 f1_macro: 0.366\n",
            "Time taken: 16.03s\n",
            "Epoch 14\n",
            "Training loss (for one batch) at step 0: 0.9974\n",
            "Training loss (for one batch) at step 50: 0.9393\n",
            "Training loss (for one batch) at step 100: 0.9443\n",
            "Training loss (for one batch) at step 150: 0.7484\n",
            "Validation acc: 0.544 precision: 0.605 recall: 0.459 f1_micro: 0.522 f1_macro: 0.376\n",
            "Time taken: 15.53s\n",
            "Epoch 15\n",
            "Training loss (for one batch) at step 0: 0.8912\n",
            "Training loss (for one batch) at step 50: 0.9765\n",
            "Training loss (for one batch) at step 100: 0.9991\n",
            "Training loss (for one batch) at step 150: 1.1246\n",
            "Validation acc: 0.519 precision: 0.610 recall: 0.363 f1_micro: 0.455 f1_macro: 0.359\n",
            "Time taken: 15.89s\n",
            "Epoch 16\n",
            "Training loss (for one batch) at step 0: 1.2369\n",
            "Training loss (for one batch) at step 50: 1.0395\n",
            "Training loss (for one batch) at step 100: 1.0381\n",
            "Training loss (for one batch) at step 150: 1.0784\n",
            "Validation acc: 0.542 precision: 0.635 recall: 0.414 f1_micro: 0.502 f1_macro: 0.341\n",
            "Time taken: 15.92s\n",
            "Epoch 17\n",
            "Training loss (for one batch) at step 0: 1.0024\n",
            "Training loss (for one batch) at step 50: 0.9366\n",
            "Training loss (for one batch) at step 100: 0.8189\n",
            "Training loss (for one batch) at step 150: 0.9296\n",
            "Validation acc: 0.554 precision: 0.668 recall: 0.409 f1_micro: 0.507 f1_macro: 0.384\n",
            "Time taken: 16.06s\n",
            "Epoch 18\n",
            "Training loss (for one batch) at step 0: 1.0821\n",
            "Training loss (for one batch) at step 50: 0.8871\n",
            "Training loss (for one batch) at step 100: 0.9666\n",
            "Training loss (for one batch) at step 150: 1.0214\n",
            "Validation acc: 0.551 precision: 0.645 recall: 0.444 f1_micro: 0.526 f1_macro: 0.370\n",
            "Time taken: 15.91s\n",
            "Epoch 19\n",
            "Training loss (for one batch) at step 0: 1.0984\n",
            "Training loss (for one batch) at step 50: 1.1195\n",
            "Training loss (for one batch) at step 100: 0.9173\n",
            "Training loss (for one batch) at step 150: 0.9241\n",
            "Validation acc: 0.551 precision: 0.636 recall: 0.450 f1_micro: 0.527 f1_macro: 0.353\n",
            "Time taken: 16.18s\n",
            "Epoch 20\n",
            "Training loss (for one batch) at step 0: 1.1185\n",
            "Training loss (for one batch) at step 50: 1.3080\n",
            "Training loss (for one batch) at step 100: 1.0418\n",
            "Training loss (for one batch) at step 150: 1.0205\n",
            "Validation acc: 0.558 precision: 0.660 recall: 0.424 f1_micro: 0.517 f1_macro: 0.374\n",
            "Time taken: 16.37s\n",
            "Epoch 21\n",
            "Training loss (for one batch) at step 0: 0.9167\n",
            "Training loss (for one batch) at step 50: 1.0558\n",
            "Training loss (for one batch) at step 100: 1.1035\n",
            "Training loss (for one batch) at step 150: 1.1499\n",
            "Validation acc: 0.540 precision: 0.612 recall: 0.421 f1_micro: 0.499 f1_macro: 0.389\n",
            "Time taken: 15.91s\n",
            "Epoch 22\n",
            "Training loss (for one batch) at step 0: 1.1130\n",
            "Training loss (for one batch) at step 50: 0.9987\n",
            "Training loss (for one batch) at step 100: 1.1643\n",
            "Training loss (for one batch) at step 150: 0.7558\n",
            "Validation acc: 0.551 precision: 0.596 recall: 0.495 f1_micro: 0.541 f1_macro: 0.357\n",
            "Time taken: 16.33s\n",
            "Epoch 23\n",
            "Training loss (for one batch) at step 0: 0.9761\n",
            "Training loss (for one batch) at step 50: 0.9881\n",
            "Training loss (for one batch) at step 100: 1.0906\n",
            "Training loss (for one batch) at step 150: 1.0009\n",
            "Validation acc: 0.550 precision: 0.651 recall: 0.409 f1_micro: 0.502 f1_macro: 0.424\n",
            "Time taken: 16.40s\n",
            "Epoch 24\n",
            "Training loss (for one batch) at step 0: 0.9616\n",
            "Training loss (for one batch) at step 50: 0.8067\n",
            "Training loss (for one batch) at step 100: 0.9764\n",
            "Training loss (for one batch) at step 150: 0.9714\n",
            "Validation acc: 0.549 precision: 0.612 recall: 0.465 f1_micro: 0.528 f1_macro: 0.386\n",
            "Time taken: 16.32s\n",
            "Epoch 25\n",
            "Training loss (for one batch) at step 0: 0.7634\n",
            "Training loss (for one batch) at step 50: 0.9704\n",
            "Training loss (for one batch) at step 100: 1.1550\n",
            "Training loss (for one batch) at step 150: 1.0184\n",
            "Validation acc: 0.564 precision: 0.630 recall: 0.453 f1_micro: 0.527 f1_macro: 0.413\n",
            "Time taken: 16.37s\n",
            "Epoch 26\n",
            "Training loss (for one batch) at step 0: 1.0731\n",
            "Training loss (for one batch) at step 50: 1.0690\n",
            "Training loss (for one batch) at step 100: 0.9610\n",
            "Training loss (for one batch) at step 150: 0.9839\n",
            "Validation acc: 0.547 precision: 0.662 recall: 0.421 f1_micro: 0.515 f1_macro: 0.426\n",
            "Time taken: 16.40s\n",
            "Epoch 27\n",
            "Training loss (for one batch) at step 0: 0.9103\n",
            "Training loss (for one batch) at step 50: 1.0474\n",
            "Training loss (for one batch) at step 100: 1.0921\n",
            "Training loss (for one batch) at step 150: 1.1013\n",
            "Validation acc: 0.542 precision: 0.623 recall: 0.418 f1_micro: 0.500 f1_macro: 0.414\n",
            "Time taken: 16.05s\n",
            "Epoch 28\n",
            "Training loss (for one batch) at step 0: 0.7455\n",
            "Training loss (for one batch) at step 50: 0.9761\n",
            "Training loss (for one batch) at step 100: 0.8725\n",
            "Training loss (for one batch) at step 150: 0.7465\n",
            "Validation acc: 0.541 precision: 0.651 recall: 0.394 f1_micro: 0.491 f1_macro: 0.353\n",
            "Time taken: 16.31s\n",
            "Epoch 29\n",
            "Training loss (for one batch) at step 0: 1.0532\n",
            "Training loss (for one batch) at step 50: 0.8868\n",
            "Training loss (for one batch) at step 100: 0.8575\n",
            "Training loss (for one batch) at step 150: 0.9334\n",
            "Validation acc: 0.531 precision: 0.620 recall: 0.381 f1_micro: 0.472 f1_macro: 0.378\n",
            "Time taken: 15.88s\n"
          ]
        }
      ],
      "source": [
        "max_val_acc = 0.0\n",
        "best_weights = None\n",
        "l_iter = iter(labeled_ds)\n",
        "u_iter = iter(unlabeled_ds)\n",
        "t_step = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch}\")\n",
        "  start_time = time.time()\n",
        "\n",
        "  for step in range(steps_per_epoch):\n",
        "    x, y = next(l_iter)\n",
        "    xu = next(u_iter)\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    lam = max([lam,1-lam])\n",
        "    blend = tf.constant(lam)\n",
        "    unsup_val = weight_schedule(t_step, init_period, final_period, max_period)\n",
        "    unsup_weight.assign(4.0 * unsup_val)\n",
        "    \n",
        "    loss = train_step(x,y,xu,unsup_weight,blend)\n",
        "\n",
        "    if step % 50 == 0:\n",
        "      print(\n",
        "        \"Training loss (for one batch) at step %d: %.4f\"\n",
        "        % (step, float(loss))\n",
        "      )\n",
        "    t_step+=1\n",
        "  for x_batch_val, y_batch_val in val_ds:\n",
        "      test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "  acc = float(val_acc_metric.result())\n",
        "  prec = float(val_prec_metric.result())\n",
        "  recall = float(val_recall_metric.result())\n",
        "  micro = float(f1_metric_micro.result())\n",
        "  macro = float(f1_metric_macro.result())\n",
        "\n",
        "  val_acc_metric.reset_states()\n",
        "  val_prec_metric.reset_states()\n",
        "  val_recall_metric.reset_states()\n",
        "  f1_metric_micro.reset_states()\n",
        "  f1_metric_macro.reset_states()\n",
        "\n",
        "  if acc > max_val_acc:\n",
        "    max_val_acc = acc\n",
        "    best_weights = model.get_weights()\n",
        "\n",
        "  print(f\"Validation acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")\n",
        "  print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "model.set_weights(best_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMVyo5UGMDDO",
        "outputId": "04b8c7f0-1823-40ce-daf4-ba3074985adc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test acc: 0.556 precision: 0.623 recall: 0.439 f1_micro: 0.515 f1_macro: 0.399\n"
          ]
        }
      ],
      "source": [
        "for x_batch_val, y_batch_val in test_ds:\n",
        "    test_step(x_batch_val, y_batch_val)\n",
        "acc = float(val_acc_metric.result())\n",
        "prec = float(val_prec_metric.result())\n",
        "recall = float(val_recall_metric.result())\n",
        "micro = float(f1_metric_micro.result())\n",
        "macro = float(f1_metric_macro.result())\n",
        "\n",
        "val_acc_metric.reset_states()\n",
        "val_prec_metric.reset_states()\n",
        "val_recall_metric.reset_states()\n",
        "f1_metric_micro.reset_states()\n",
        "f1_metric_macro.reset_states()\n",
        "print(f\"Test acc: {acc:.3f} precision: {prec:.3f} recall: {recall:.3f} f1_micro: {micro:.3f} f1_macro: {macro:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHxbtb2ydqlW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
